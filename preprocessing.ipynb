{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecbf2ee",
   "metadata": {},
   "source": [
    "# Token classification\n",
    "This notebook shows our approach to the data preprocessing. The goal is to have exactly one label per token*, including an \"empty\" label.\n",
    "As long as we restrict our data to only one predicate, it should be feasible to determine two what other part of the sentence the role connects to.\n",
    "\n",
    "\\* In this step, token refers to the \"tokenization\" as applied to the PMB, i.e. the tokens in the \"en.tok.off\" files. E.g., \"Alfred Nobel\" is one token here.\n",
    "Our LLM will tokenize our sentence differently, and will create one or more tokens per PMB token. This mapping will be handled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b43fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ROLES/LABELS: Agent, Location, Topic, Patient, Theme, EMPTY\n",
    "# Tags: 0=EMPTY, 1=Agent, 2=Location, 3=Patient, 4=Theme, 5=Topic\n",
    "\n",
    "# sentence = \"A brown dog and a grey dog are fighting in the snow\"\n",
    "# The goal is to generate:\n",
    "# srl_tags = [1,1,1,1,1,1,1,0,0,2,2,2]b\n",
    "# tokens = ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95a0d7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "# Example with one sentence:\n",
    "# Note: forward slashes for Linux and WSL, backward slashes for Windows\n",
    "# Windows example:\n",
    "# file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004'\n",
    "# WSL example:\n",
    "# file_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0004/'\n",
    "# file_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p01/d2590/' # https://pmb.let.rug.nl/explorer/explore.php?part=01&doc_id=2590&type=der.xml&alignment_language=en\n",
    "file_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p03/d0766/' # https://pmb.let.rug.nl/explorer/explore.php?part=03&doc_id=0766&type=der.xml&alignment_language=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05f50e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred Nobel invented dynamite in 1866 .\n",
      "['Alfred Nobel', 'invented', 'dynamite', 'in', '1866', '.']\n"
     ]
    }
   ],
   "source": [
    "# THIS IS THE GOAL\n",
    "# sentence = \"A brown dog and a grey dog are fighting in the snow\"\n",
    "mapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5, \"Destination\": 6, \"Result\": 7}\n",
    "\n",
    "sentence = \"\"\n",
    "sentence_id = '0'\n",
    "tokens = []\n",
    "\n",
    "# Get the tokens from the tokenized sentence file\n",
    "with open(file_path+\"en.tok.off\") as file:\n",
    "    for line in file:\n",
    "        tokens.append(line.split(maxsplit = 3)[-1].rstrip())\n",
    "\n",
    "sentence = ' '.join(tokens)\n",
    "\n",
    "print(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2342f5",
   "metadata": {},
   "source": [
    "## Our class-based approach\n",
    "We take the en.parse.tags file and recreate the CCG structure using custom classes.\n",
    "This allows us to figure out to what tokens each semantic role label belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9670a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CCGNode:\n",
    "    def __init__(self, category = 'none', rule_type='none', parent=None, level = 0):\n",
    "        self.category = category # eg s\\np or np\n",
    "        self.rule_type = rule_type # fa or ba or conj\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.level = level\n",
    "        self.isFirstArgument = True\n",
    "    \n",
    "    def addChild(self, child):\n",
    "        if len(self.children) == 1:\n",
    "            child.isFirstArgument = False\n",
    "        elif len(self.children) == 2:\n",
    "            raise Exception(repr(self), 'already has two children')\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def getSibling(self):\n",
    "        if self.isFirstArgument:\n",
    "            return self.parent.children[1]\n",
    "        else:\n",
    "            return self.parent.children[0]\n",
    "    \n",
    "    def assignTag(self, tag):\n",
    "        self.children[0].assignTag(tag)\n",
    "        if len(self.children) > 1:\n",
    "            self.children[1].assignTag(tag)\n",
    "    \n",
    "    def getTags(self, mapping = None):\n",
    "        if len(self.children) == 1:\n",
    "            return self.children[0].getTags(mapping)\n",
    "        return self.children[0].getTags(mapping) + self.children[1].getTags(mapping)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([' ' * self.level, 'CCGNODE', ' ', self.category, ' ', self.rule_type, '\\n', '\\n'.join([repr(child) for child in self.children])])\n",
    "\n",
    "class CCGToken:\n",
    "    def __init__(self, token, category, parent, assignedTag = '', verbnet = [], level = 0):\n",
    "        self.token = token\n",
    "        self.category = category\n",
    "        self.parent = parent\n",
    "        self.assignedTag = assignedTag\n",
    "        self.verbnet = verbnet\n",
    "        self.children = []\n",
    "        self.level = level\n",
    "        self.isFirstArgument = True\n",
    "        \n",
    "    def getSibling(self):\n",
    "        if self.isFirstArgument:\n",
    "            return self.parent.children[1]\n",
    "        else:\n",
    "            return self.parent.children[0]\n",
    "    \n",
    "    def assignTag(self, tag):\n",
    "        self.assignedTag = tag\n",
    "    \n",
    "    def getTags(self, mapping):\n",
    "        if mapping == None:\n",
    "            return [self.assignedTag]\n",
    "        else:\n",
    "            if self.assignedTag == '':\n",
    "                return [0]\n",
    "            return [mapping[self.assignedTag]]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([' ' * self.level, 'CCGTOKEN', ' ', self.token, ' ', self.category, ' ', self.assignedTag, ' ',' '.join(self.verbnet)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca72674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(file_path):\n",
    "    tokens = []\n",
    "    # Get the tokens from the tokenized sentence file\n",
    "    with open(os.path.join(file_path, \"en.tok.off\")) as file:\n",
    "        for line in file:\n",
    "            tokens.append(line.split(maxsplit = 3)[-1].rstrip())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30a4021f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getTree(file_path, tokens):\n",
    "    token_idx = 0\n",
    "    topNode = None\n",
    "    currentNode = None\n",
    "    tokensWithVerbnet = []\n",
    "    with open(os.path.join(file_path, \"en.parse.tags\")) as file:\n",
    "        skipping = True\n",
    "        previousLevel = 0\n",
    "        for line in file:\n",
    "            if skipping:\n",
    "                if line.startswith('ccg'):\n",
    "                    skipping = False\n",
    "                    topNode = CCGNode()\n",
    "                    currentNode = topNode\n",
    "                continue\n",
    "            if line == '\\n':\n",
    "                continue\n",
    "            trimmedLine = line.lstrip()\n",
    "            nodeType, content = trimmedLine.split('(', 1)\n",
    "            category = content.split(',')[0]\n",
    "            if nodeType == 't':\n",
    "                if category == '.':\n",
    "                    continue\n",
    "                vnSplit = content.split(\"verbnet:\")\n",
    "                if len(vnSplit) == 1:\n",
    "                    verbnet = []\n",
    "                else:\n",
    "                    # It needs to combine to an np. Verbnet tags looking for a n for example,\n",
    "                    # often describe adjectives and are not relevant for the main predicate\n",
    "                    searchingFor = re.split(r'[\\\\\\/]', category, 1)\n",
    "                    if \"np\" in searchingFor[1]:\n",
    "                        verbnetLiteral = vnSplit[1].split(']')[0] + ']'\n",
    "                        verbnetUnfiltered = eval(verbnetLiteral)\n",
    "                        verbnet = [r for r in verbnetUnfiltered if r in mapping.keys()]\n",
    "\n",
    "                currentNode.addChild(CCGToken(tokens[token_idx], category = category, parent = currentNode, verbnet = verbnet, level = currentNode.level + 1))\n",
    "                if len(verbnet) > 0:\n",
    "                    tokensWithVerbnet.append(currentNode.children[-1])\n",
    "                token_idx += 1\n",
    "            else:\n",
    "                level = len(line) - len(trimmedLine)\n",
    "                if level > previousLevel: # This is a child of previous node\n",
    "                    currentNode.addChild(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                    currentNode = currentNode.children[-1]\n",
    "                elif level == previousLevel: # Sibling of the previous node; same parent\n",
    "                    currentNode = currentNode.parent\n",
    "                    currentNode.addChild(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                    currentNode = currentNode.children[-1]\n",
    "                else: # Go back 1 or more levels\n",
    "                    while not currentNode.isFirstArgument:\n",
    "                        currentNode = currentNode.parent\n",
    "                    currentNode = currentNode.parent\n",
    "                    currentNode.addChild(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                    currentNode = currentNode.children[-1]\n",
    "\n",
    "                previousLevel = level\n",
    "\n",
    "    return topNode, tokensWithVerbnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05f37846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: deal with multiple verbnet labels.\n",
    "\n",
    "def findCorrectLevel(current):\n",
    "    while (not current.category.endswith('np')): # first application with non-nps\n",
    "        current = current.parent\n",
    "    lookingForward = (current.category[-3] == '/')\n",
    "    if lookingForward:\n",
    "        while ((not current.isFirstArgument) or current.parent.rule_type != 'fa'):\n",
    "            current = current.parent\n",
    "    else:\n",
    "        while (current.isFirstArgument or current.parent.rule_type != 'ba'):\n",
    "            current = current.parent\n",
    "    return current\n",
    "\n",
    "def assignTags(tokensWithVerbnet):\n",
    "    for currentTokenWithVerbnet in tokensWithVerbnet:\n",
    "        verbnet = currentTokenWithVerbnet.verbnet\n",
    "        for verbnetItem in verbnet:\n",
    "            currentTokenWithVerbnet = findCorrectLevel(currentTokenWithVerbnet)\n",
    "            sibling = currentTokenWithVerbnet.getSibling()\n",
    "            sibling.assignTag(verbnetItem)\n",
    "            currentTokenWithVerbnet = currentTokenWithVerbnet.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc9980d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np fa\n",
      "   CCGTOKEN The np/n  \n",
      "   CCGTOKEN yakuza n  \n",
      "  CCGNODE s:dcl\\np fa\n",
      "   CCGTOKEN are (s:dcl\\np)/np  Theme\n",
      "   CCGNODE np rp\n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN the np/n  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN Japanese n/n  \n",
      "      CCGTOKEN mafia n  \n",
      "['The', 'yakuza', 'are', 'the', 'Japanese', 'mafia', '.']\n",
      "[0, 0, 0, 4, 4, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "mapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5, \"Destination\": 6, \"Result\": 7}\n",
    "\n",
    "file_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0717/'\n",
    "# file_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0004/'\n",
    "\n",
    "def getTokensAndLabels(file_path):\n",
    "    tokens = getTokens(file_path)\n",
    "    topNode, tokensWithVerbnet = getTree(file_path, tokens)\n",
    "    print(topNode)\n",
    "    assignTags(tokensWithVerbnet)\n",
    "    labels = topNode.getTags(mapping)\n",
    "    if tokens[-1] != '.':\n",
    "        tokens.append('.')\n",
    "    labels.append(0)\n",
    "    return tokens, labels\n",
    "    \n",
    "\n",
    "tokens, labels = getTokensAndLabels(file_path)\n",
    "print(tokens)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69adfafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0004\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0028\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0054\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0055\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0123\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0182\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0240\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0712\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0715\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0716\n",
      "/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00/d0717\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-87c368d0c426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-87c368d0c426>\u001b[0m in \u001b[0;36mcreateDataset\u001b[0;34m(parent_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTokensAndLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a0db8c0fd8b8>\u001b[0m in \u001b[0;36mgetTokensAndLabels\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtopNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokensWithVerbnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0massignTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokensWithVerbnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-b61db7833643>\u001b[0m in \u001b[0;36massignTags\u001b[0;34m(tokensWithVerbnet)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mverbnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentTokenWithVerbnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mverbnetItem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverbnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mcurrentTokenWithVerbnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindCorrectLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentTokenWithVerbnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0msibling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentTokenWithVerbnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSibling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massignTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbnetItem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-b61db7833643>\u001b[0m in \u001b[0;36mfindCorrectLevel\u001b[0;34m(current)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'np'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# first application with non-nps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlookingForward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlookingForward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misFirstArgument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrule_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'fa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "folder_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-4.0.0/data/en/gold/p00' \n",
    "\n",
    "def createDataset(parent_dir):\n",
    "    dataset = {'tokens': [], 'labels': []}\n",
    "    for subdir, dirs, files in os.walk(parent_dir):\n",
    "        if not os.path.exists(os.path.join(subdir, 'en.parse.tags')):\n",
    "            continue\n",
    "        print(subdir)\n",
    "        tokens, labels = getTokensAndLabels(subdir)\n",
    "        dataset['tokens'].append(tokens)\n",
    "        dataset['labels'].append(labels)\n",
    "    return dataset\n",
    "        \n",
    "\n",
    "dataset = createDataset(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f97c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "436bbde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np/np2/np3\n",
      "['np', 'np2/np3']\n"
     ]
    }
   ],
   "source": [
    "test = \"np/np2/np3\"\n",
    "print(test)\n",
    "\n",
    "result = re.split(r'[\\\\\\/]', test, 1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf2aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
