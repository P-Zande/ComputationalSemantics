{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86af874d",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292239ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_column(example):\n",
    "    example[\"ner_tags\"] = example[\"labels\"]\n",
    "    return example\n",
    "\n",
    "data = load_from_disk(\"dataset.hf\")\n",
    "id_column = range(data.num_rows)\n",
    "data = data.add_column(\"id\", id_column)\n",
    "data = data.map(new_column)\n",
    "\n",
    "# Split up the data for testing and training\n",
    "data = data.train_test_split(test_size=0.1)\n",
    "test_data = data[\"test\"]\n",
    "data = data[\"train\"].train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da777f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx]) # Label all tokens of a given word\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data = data.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test_data = test_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a feel of how the data looks like\n",
    "\n",
    "print(data)\n",
    "\n",
    "example = data[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "print(tokenized_data[\"train\"][0])\n",
    "print(\"The input will be tokenized as:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201119dc",
   "metadata": {},
   "source": [
    "## Evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_group_lists(first_list, second_list):\n",
    "    result_first = []\n",
    "    result_second = []\n",
    "    previous_value = None\n",
    "    current_group = []\n",
    "\n",
    "    for value1, value2 in zip(first_list, second_list):\n",
    "        if value1 != -100 and value1 != 0:\n",
    "            if value1 != previous_value:\n",
    "                if current_group:\n",
    "                    result_second.append(current_group)\n",
    "                current_group = [value2]\n",
    "                result_first.append(value1)\n",
    "            else:\n",
    "                current_group.append(value2)\n",
    "        previous_value = value1\n",
    "\n",
    "    if current_group:\n",
    "        result_second.append(current_group)\n",
    "\n",
    "    return result_first, result_second\n",
    "\n",
    "# As an example of how this function works:\n",
    "# first_list = [-100, 2, 0, 1, 1, 0, 0, 2, 0, 1, 2, -100]\n",
    "# second_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "\n",
    "# filtered_first, grouped_second = filter_and_group_lists(first_list, second_list)\n",
    "# print(\"Filtered First List:\", filtered_first)\n",
    "# print(\"Grouped Second List:\", grouped_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalPrediction(predictions):\n",
    "    predictions = np.array(predictions)\n",
    "    non_zero_predictions = predictions[predictions != 0]\n",
    "    counts = np.bincount(non_zero_predictions)\n",
    "    if counts.size == 0:\n",
    "        return 0\n",
    "    return np.argmax(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "#mapping = {\"Theme\": 1, \"Agent\": 2, \"Patient\": 3, \"Experiencer\": 4, \"Co-Theme\": 5, \"Stimulus\": 6, \"Location\": 7, \"Destination\": 8}\n",
    "label_list = [\n",
    "    \"O\",\n",
    "    \"Theme\",\n",
    "    \"Agent\",\n",
    "    \"Patient\",\n",
    "    \"Experiencer\",\n",
    "    \"Co-Theme\",\n",
    "    \"Stimulus\",\n",
    "    \"Location\",\n",
    "    \"Destination\",\n",
    "]\n",
    "\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "def getTrueLabelsAndPredictions(labels, predictions):\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        true_label, grouped_predictions = filter_and_group_lists(label, prediction)\n",
    "\n",
    "        true_labels.append([label_list[l] for l in true_label])\n",
    "        true_prediction = list(map(getFinalPrediction, grouped_predictions))\n",
    "\n",
    "        true_predictions.append([label_list[p] for p in true_prediction])\n",
    "    \n",
    "    return true_labels, true_predictions\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_labels, true_predictions = getTrueLabelsAndPredictions(labels, predictions)\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af076a6f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"Theme\": 1, \"Agent\": 2, \"Patient\": 3, \"Experiencer\": 4, \"Co-Theme\": 5, \"Stimulus\": 6, \"Location\": 7, \"Destination\": 8}\n",
    "\n",
    "label2id = {\"O\": 0}\n",
    "label2id.update(mapping)\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=9, id2label=id2label, label2id=label2id\n",
    ")\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"thematic_role_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=4,\n",
    "#     load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8fd2d",
   "metadata": {},
   "source": [
    "# Test on separate part of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fe4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForTokenClassification.from_pretrained(\"thematic_role_model/checkpoint-832\")\n",
    "testing_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=100,  # Adjust as needed\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=trained_model,\n",
    "    args=testing_args,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=tokenized_data[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712ea96",
   "metadata": {},
   "source": [
    "# Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_test_predictions = trainer.predict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf62ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "test_predictions = np.argmax(raw_test_predictions.predictions, axis=2)\n",
    "true_test_labels, true_test_predictions = getTrueLabelsAndPredictions(tokenized_test_data['labels'], test_predictions)\n",
    "true_test_labels = flatten(true_test_labels)\n",
    "true_test_predictions = flatten(true_test_predictions)\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "matrix = ConfusionMatrixDisplay.from_predictions(y_true = true_test_labels, \n",
    "                                                 y_pred = true_test_predictions,\n",
    "                                                 xticks_rotation = 'vertical',\n",
    "                                                 ax = ax)\n",
    "\n",
    "\n",
    "plt.savefig(\"confusion.png\", dpi = 600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9aa01",
   "metadata": {},
   "source": [
    "# Inference\n",
    "This block performs inference on a given sentence. Note that it returns a label per Bert token. There is always a special token at the beginning and end of each sentence. The predicted labels for these tokens have been removed. For most simple sentences, one word corresponds to one Bert token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5540636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# text = \"I deserve to know the truth.\"\n",
    "text = \"Tom didn't know when Mary had come to Boston.\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thematic_role_model/checkpoint-832\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"thematic_role_model/checkpoint-832\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "\n",
    "print(text)\n",
    "print(predicted_token_class[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6b5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
