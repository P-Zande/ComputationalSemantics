{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecbf2ee",
   "metadata": {},
   "source": [
    "# Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b43fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROLES/LABELS: Agent, Location, Topic, Patient, Theme, EMPTY\n",
    "# Tags: 0=EMPTY, 1=Agent, 2=Location, 3=Patient, 4=Theme, 5=Topic\n",
    "\n",
    "# Generate:\n",
    "# Whole sentence +\n",
    "# {'id': '0',\n",
    "#  'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#  'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n",
    "# }\n",
    "\n",
    "# THIS IS THE GOAL\n",
    "# Tags: 0=EMPTY, 1=Agent, 2=Location, 3=Patient, 4=Theme, 5=Topic\n",
    "# ner_tags = [1,1,1,1,1,1,1,0,0,2,2,2]\n",
    "# tokens = ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bcdd3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    t(np/n, 'A', [from:0, to:1, pos:'DT', lemma:'a', sem:'DIS', wordnet:'O']),\n",
      "\n",
      "     t(n/n, 'brown', [from:2, to:7, pos:'JJ', lemma:'brown', sem:'COL', wordnet:'brown.a.01', verbnet:['Colour']]),\n",
      "\n",
      "     t(n, 'dog', [from:8, to:11, pos:'NN', lemma:'dog', sem:'CON', wordnet:'dog.n.01']))),\n",
      "\n",
      "    t(conj, 'and', [from:12, to:15, pos:'CC', lemma:'and', sem:'GRP', wordnet:'O']),\n",
      "\n",
      "    fa(np,\n",
      "\n",
      "      t(n/n, 'grey', [from:18, to:22, pos:'NN', lemma:'grey', sem:'COL', wordnet:'grey.a.01', verbnet:['Colour']]),\n",
      "\n",
      "      t(n, 'dog', [from:23, to:26, pos:'NN', lemma:'dog', sem:'CON', wordnet:'dog.n.01']))))),\n",
      "\n",
      "    t((s:dcl\\np)/(s:ng\\np), 'are', [from:27, to:30, pos:'VBP', lemma:'be', sem:'NOW', wordnet:'O']),\n",
      "\n",
      "    t(s:ng\\np, 'fighting', [from:31, to:39, pos:'VBG', lemma:'fight', sem:'EXG', wordnet:'fight.v.01', verbnet:['Agent']])),\n",
      "\n",
      "Has role: ['Agent']\n",
      "CCG (last part): \\np\n",
      "backwards\n",
      "    t(((s\\np)\\(s\\np))/np, 'in', [from:40, to:42, pos:'IN', lemma:'in', sem:'REL', wordnet:'O', verbnet:['Location']]),\n",
      "\n",
      "Has role: ['Location']\n",
      "CCG (last part): /np\n",
      "     t(np/n, 'the', [from:43, to:46, pos:'DT', lemma:'the', sem:'DEF', wordnet:'O']),\n",
      "\n",
      "     t(n, 'snow', [from:47, to:51, pos:'NN', lemma:'snow', sem:'CON', wordnet:'snow.n.02'])))))).\n",
      "\n",
      "NPs and their tokens: [['ba(np'], ['fa(np', 'A', 'brown', 'dog', 'and'], ['fa(np', 'a', 'grey', 'dog', 'are', 'fighting', 'in'], ['fa(np', 'the', 'snow']]\n",
      "tokens: ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']\n",
      "sentence: A brown dog and a grey dog are fighting in the snow\n",
      "ner_tags: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Example with one sentence:\n",
    "file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004'\n",
    "\n",
    "# THIS IS THE GOAL\n",
    "# sentence = \"A brown dog and a grey dog are fighting in the snow\"\n",
    "# sentence_id = '0'\n",
    "# ner_tags = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2]\n",
    "# tokens = ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']\n",
    "mapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5}\n",
    "\n",
    "sentence = \"\"\n",
    "sentence_id = '0'\n",
    "ner_tags = [],\n",
    "tokens = []\n",
    "\n",
    "# Get the tokens from the tokenized sentence file\n",
    "with open(file_path+\"\\en.tok.off\") as file:\n",
    "    for line in file:\n",
    "        tokens.append(line.split()[-1])\n",
    "\n",
    "sentence = ' '.join(tokens)\n",
    "# Initially set all the tags as 0 (EMPTY)\n",
    "ner_tags = [0] * len(tokens)\n",
    "    \n",
    "# get the roles for each token from the (parse/drs?)\n",
    "token_idx = 0\n",
    "with open(file_path+\"\\en.parse.tags\") as file:\n",
    "    # collect all the NP and corresponding words\n",
    "    NPs = []\n",
    "    for line in file:\n",
    "        if re.search('([fb]a\\(np),', line):\n",
    "            NPs.append([re.search('([fb]a\\(np),', line).group(1)])\n",
    "        \n",
    "        # Find a CCG for a token\n",
    "        if token_idx < len(tokens) and line.find(tokens[token_idx]) != -1: # TODO: fix to make sure single letter/small words found correctly\n",
    "            print(line)\n",
    "#             print(\"Contains token:\", tokens[token_idx], \"at position\", line.find(tokens[token_idx]))\n",
    "            \n",
    "            # Add the token to its closes NP\n",
    "            NPs[-1].append(tokens[token_idx])\n",
    "            \n",
    "            # Check whether any of the roles are in the sentence\n",
    "            roles = [r for r in mapping.keys() if r in line]\n",
    "            if roles:\n",
    "#                 print(line)\n",
    "                print(\"Has role:\", roles)\n",
    "                \n",
    "                # Find the CCG (using regex after \"t(\", finding last \\np or /np before ,)\n",
    "                ccg = re.search('([\\\\/\\\\\\][np]+),', line) # TODO: make sure I didn't think too simple for this\n",
    "                ccg = ccg.group(1)\n",
    "                print(\"CCG (last part):\", ccg)\n",
    "                \n",
    "                # Find whether role should be placed using forward/backward application\n",
    "                if ccg[0] == \"\\\\\": #backwards\n",
    "                    print(\"backwards\")\n",
    "                    # Look at all previous \"ba(np\" and find the one corresponding to this ccg\n",
    "                elif ccg[0] == \"\\/\": #forwards\n",
    "                    print(\"forwards\")\n",
    "                    # Look at all next \"fa(np\" and find the one corresponding to this ccg\n",
    "                    \n",
    "                # Find place of NP for role (\"fa(np\" or \"ba(np\")\n",
    "                # Find all word positions belonging to NP and assign their tokens the correct role?\n",
    "                # TODO: make sure it is fine to only look at NPs\n",
    "            token_idx += 1\n",
    "    print(\"NPs and their tokens:\", NPs)\n",
    "\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"ner_tags:\", ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fcbc7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(6, 10), match='\\\\np,'>\n",
      "\\np\n"
     ]
    }
   ],
   "source": [
    "txt = r\"t(s:ng\\np, 'fighting', [from:31, to:39, pos:'VBG', lemma:'fight', sem:'EXG', wordnet:'fight.v.01', verbnet:['Agent']])),\"\n",
    "print(re.search('([\\\\/\\\\\\][np]+),', txt))\n",
    "print(re.search('([\\\\/\\\\\\][np]+),', txt).group(1))\n",
    "# print(re.search('([\\\\/\\\\\\][np]+),', \"r\\'\"+txt+\"\\'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427c08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd4e4d5",
   "metadata": {},
   "source": [
    "# Graph/DRS data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33269896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31fface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text: brown.a.01 dog.n.01 Colour -1 entity.n.01 Sub -1 Sub +2 grey.a.01 dog.n.01 Colour -1 time.n.08 EQU now fight.v.01 Agent -4 Time -1 Location +1 snow.n.02 \n",
      "The roles masked are: ['Colour', 'Sub', 'Sub', 'Colour', 'Agent', 'Time', 'Location']\n",
      "\n",
      "Fully masked text: brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "\n",
      "Text with [MASK] for roles:\n",
      "brown.a.01 dog.n.01 [MASK] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [MASK] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [MASK] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [MASK] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [MASK] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [MASK] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [MASK] +1 snow.n.02 \n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(file_path, save_path):\n",
    "    file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "    save_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\sentences.csv'\n",
    "    processed_text = \"\"\n",
    "    with open(file_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] != \"%\":\n",
    "                text = line\n",
    "                # remove extra whitespace\n",
    "                text = \" \".join(text.split()).strip()\n",
    "                # remove comments\n",
    "                comment_start = text.find(\"%\")\n",
    "                if comment_start > -1:\n",
    "                    text = text[:comment_start]\n",
    "                processed_text += text\n",
    "    print(\"Processed text:\", processed_text)\n",
    "    \n",
    "    # method to replace roles with \"[X]\" or [MASK]\n",
    "    roles = [] # fill in list when only specific roles (so, not Sub etc.)\n",
    "    if not roles:\n",
    "        # find roles by using -1 and +2, etc.\n",
    "        roles = re.findall('\\d (\\w+) [-+]', processed_text)\n",
    "        print(\"The roles masked are:\", roles)\n",
    "        \n",
    "    final_text = processed_text\n",
    "    for role in roles:\n",
    "        final_text = final_text.replace(role, \"[X]\")\n",
    "    print(\"\\nFully masked text:\", final_text)\n",
    "    \n",
    "    print(\"\\nText with [MASK] for roles:\")\n",
    "    instances = [m.start()-1 for m in re.finditer(\"[X]\", final_text)]\n",
    "    for instance in instances:\n",
    "        before = final_text[:instance]\n",
    "        after = final_text[instance:].replace(\"[X]\", \"[MASK]\", 1)\n",
    "        masked_text = before + after\n",
    "        print(masked_text)\n",
    "        \n",
    "preprocess_sentence(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51894c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Need to find: [\"Colour\", \"Sub\", \"Sub\", \"Colour\", \"Agent\", \"Time\", \"Location\"]\n",
    "s = 'brown.a.01 dog.n.01 Colour -1 entity.n.01 Sub -1 Sub +2 grey.a.01 dog.n.01 Colour -1 time.n.08 EQU now fight.v.01 Agent -4 Time -1 Location +1 snow.n.02 '\n",
    "\n",
    "# result = re.findall('\\d (.*?) [-+]\\d', s)\n",
    "# print(result)\n",
    "result = re.findall('\\d (\\w+) [-+]', s)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#letters = [\"Alef\", \"Ayin\", \"Bet\", \"Dalet\", \"Gimel\", \"He\", \"Het\", \"Kaf\", \"Kaf-final\", \"Lamed\", \"Mem\", \"Mem-medial\", \"Nun-final\", \"Nun-medial\", \"Pe\", \"Pe-final\", \"Qof\", \"Resh\", \"Samekh\", \"Shin\", \"Taw\", \"Tet\", \"Tsadi-final\", \"Tsadi-medial\", \"Waw\", \"Yod\", \"Zayin\"]\n",
    "# os.chdir(cwd)\n",
    "# r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "original_dataset_path = \"pmb-sample-4.0.0\\data\\en\\gold\"\n",
    "output_path = \"augment\"\n",
    "\n",
    "for classname in os.listdir(original_dataset_path):\n",
    "    class_input_dir = os.path.join(original_dataset_path, classname)\n",
    "    class_output_dir = os.path.join(output_path, classname)\n",
    "    # Create a new directory if it did not yet exist\n",
    "    if not os.path.exists(class_output_dir):\n",
    "        os.makedirs(class_output_dir)\n",
    "    nr = 0\n",
    "    size = len(os.listdir(class_input_dir))\n",
    "    for filename in os.listdir(class_input_dir):\n",
    "        if nr % int(size/10) == 0:\n",
    "            print(nr, \" out of \", size)\n",
    "        if True:#try:\n",
    "            f = os.path.join(class_input_dir,filename)\n",
    "            if os.path.isfile(f):\n",
    "                nr += 1\n",
    "                augment(f, classname, nr, class_output_dir)\n",
    "                cv2.destroyAllWindows()\n",
    "#                 break\n",
    "#         except:\n",
    "#             print(\"error\")\n",
    "#             break\n",
    "#             print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66630635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28e0cf94",
   "metadata": {},
   "source": [
    "# Fill-mask with specified masks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce344ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m mask_indices \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked_indices\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    109\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 111\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1380\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()  \u001b[38;5;66;03m# -100 index = padding token\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     masked_lm_loss \u001b[38;5;241m=\u001b[39m loss_fct(prediction_scores\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1383\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, sentences, labels):\n",
    "#         self.sentences = sentences\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ids = self.sentences[idx]['input_ids']\n",
    "#         labels = self.labels[idx]\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         input_ids_tensor = torch.tensor(input_ids)\n",
    "#         labels_tensor = torch.tensor(labels)\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': input_ids_tensor,\n",
    "#             'labels': labels_tensor\n",
    "#         }\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define your dataset class\n",
    "class MaskedSentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, masked_indices, labels):\n",
    "        self.sentences = sentences\n",
    "        self.masked_indices = masked_indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.sentences_idx = tokenizer.encode(self.sentences[idx], add_special_tokens=True)\n",
    "        self.masked_indices_idx = self.masked_indices[idx]\n",
    "        self.labels_idx = tokenizer.encode(self.labels[idx], add_special_tokens=True)\n",
    "        \n",
    "#         self.sentences_tensor = torch.tensor(self.sentences_idx)\n",
    "#         self.masked_indices_tensor = torch.tensor(self.masked_indices_idx)\n",
    "#         self.labels_tensor = torch.tensor(self.labels_idx)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.sentences_idx),\n",
    "            'masked_indices': torch.tensor(self.masked_indices_idx),\n",
    "            'labels': torch.tensor(self.labels_idx)\n",
    "        }\n",
    "\n",
    "# Example data\n",
    "sentences = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
    "masked_indices = [[2], [1]]\n",
    "labels = [\"masked\", \"example\"]\n",
    "\n",
    "# Tokenize input sentences\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create dataset\n",
    "dataset = MaskedSentenceDataset(sentences, masked_indices, labels)\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define DataLoader and other training parameters\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    masked_indices = [item['masked_indices'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    # Adjust the padding_value according to your tokenizer\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'masked_indices': masked_indices,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you already have your MaskedSentenceDataset instance 'dataset'\n",
    "batch_size = 32  # Adjust the batch size according to your preferences\n",
    "\n",
    "# Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        mask_indices = batch['masked_indices']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation and optimization steps\n",
    "\n",
    "# Evaluation and inference steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1545381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the is blue. underside. underside. underside. underside. underside. underside. underside\n"
     ]
    }
   ],
   "source": [
    "# # Input sentence with a mask token [MASK]\n",
    "# input_sentence = \"The [MASK] is blue.\"\n",
    "\n",
    "# # Tokenize the input sentence\n",
    "# tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "\n",
    "\n",
    "# # Get the position of the masked token in the input\n",
    "# mask_token_index = (tokenized_input['input_ids'] == tokenizer.mask_token_id).nonzero().item()\n",
    "\n",
    "# # Forward pass to get predictions\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**tokenized_input)\n",
    "\n",
    "# # Get the predicted logits for the masked token\n",
    "# predictions = outputs.logits[:, mask_token_index, :]\n",
    "\n",
    "# # Get the predicted token ID (argmax or sampling)\n",
    "# predicted_token_id = torch.argmax(predictions).item()\n",
    "\n",
    "# # Convert the predicted token ID back to a word\n",
    "# predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "\n",
    "# print(f\"The predicted word for the mask is: {predicted_word}\")\n",
    "\n",
    "# model_inputs = tokenizer([\"ما لون السماء؟\"], return_tensors=\"pt\")\n",
    "\n",
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "model_inputs = tokenizer([s3], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ca7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b480360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79cd26c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "the cat on the mat. nothing. nothing.... nothing...\n",
      "Epoch: 100\n",
      "the cat on the mat............\n",
      "Epoch: 200\n",
      "the cat on the mat............\n",
      "Epoch: 300\n",
      "the cat on the mat............\n",
      "Epoch: 400\n",
      "the cat on the mat............\n",
      "Epoch: 500\n",
      "the cat on the mat. mom. mom. mom. and mom. mom.\n",
      "Epoch: 600\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 700\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 800\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 900\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 1000\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 1100\n",
      "the cat on the mat. mom. mom. \" \". \".. \"\n",
      "Epoch: 1200\n",
      "the cat on the mat. mom. mom. \" \" \" \" \" \" \"\n",
      "Epoch: 1300\n",
      "the cat on the mat............\n",
      "Epoch: 1400\n",
      "the cat on the mat............\n",
      "Epoch: 1500\n",
      "the cat on the mat............\n",
      "Epoch: 1600\n",
      "the cat on the mat............\n",
      "Epoch: 1700\n",
      "the cat on the mat............\n",
      "Epoch: 1800\n",
      "the cat on the mat............\n",
      "Epoch: 1900\n",
      "the cat on the mat............\n",
      "Epoch: 2000\n",
      "the cat on the mat............\n",
      "Epoch: 2100\n",
      "the cat on the mat............\n",
      "Epoch: 2200\n",
      "the cat on the mat............\n",
      "Epoch: 2300\n",
      "the cat on the mat............\n",
      "Epoch: 2400\n",
      "the cat on the mat............\n",
      "Epoch: 2500\n",
      "the cat on the mat............\n",
      "Epoch: 2600\n",
      "the cat on the mat............\n",
      "Epoch: 2700\n",
      "the cat on the mat............\n",
      "Epoch: 2800\n",
      "the cat on the mat............\n",
      "Epoch: 2900\n",
      "the cat on the mat............\n",
      "Epoch: 3000\n",
      "the cat on the mat............\n",
      "Epoch: 3100\n",
      "the cat on the mat............\n",
      "Epoch: 3200\n",
      "the cat on the mat............\n",
      "Epoch: 3300\n",
      "the cat on the mat............\n",
      "Epoch: 3400\n",
      "the cat on the mat. and..........\n",
      "Epoch: 3500\n",
      "the cat on the mat............\n",
      "Epoch: 3600\n",
      "the cat on the mat. and..........\n",
      "Epoch: 3700\n",
      "the cat on the mat............\n",
      "Epoch: 3800\n",
      "the cat on the mat............\n",
      "Epoch: 3900\n",
      "the cat on the mat. and..........\n",
      "Epoch: 4000\n",
      "the cat on the mat............\n",
      "Epoch: 4100\n",
      "the cat on the mat. and..........\n",
      "Epoch: 4200\n",
      "the cat on the mat. and..........\n",
      "Epoch: 4300\n",
      "the cat on the mat............\n",
      "Epoch: 4400\n",
      "the cat on the mat. and..........\n",
      "Epoch: 4500\n",
      "the cat on the mat. and..........\n",
      "Epoch: 4600\n",
      "the cat on the mat............\n",
      "Epoch: 4700\n",
      "the cat on the mat............\n",
      "Epoch: 4800\n",
      "the cat on the mat. and..........\n",
      "Epoch: 4900\n",
      "the cat on the mat. and..........\n",
      "Epoch: 5000\n",
      "the cat on the mat............\n",
      "Epoch: 5100\n",
      "the cat on the mat............\n",
      "Epoch: 5200\n",
      "the cat on the mat. and..........\n",
      "Epoch: 5300\n",
      "the cat on the mat. and..........\n",
      "Epoch: 5400\n",
      "the cat on the mat. and..........\n",
      "Epoch: 5500\n",
      "the cat on the mat............\n",
      "Epoch: 5600\n",
      "the cat on the mat............\n",
      "Epoch: 5700\n",
      "the cat on the mat. and..........\n",
      "Epoch: 5800\n",
      "the cat on the mat. and..........\n",
      "Epoch: 5900\n",
      "the cat on the mat. and..........\n",
      "Epoch: 6000\n",
      "the cat on the mat. and..........\n",
      "Epoch: 6100\n",
      "the cat on the mat. and. \" \".......\n",
      "Epoch: 6200\n",
      "the cat on the mat............\n",
      "Epoch: 6300\n",
      "the cat on the mat............\n",
      "Epoch: 6400\n",
      "the cat on the mat. and..........\n",
      "Epoch: 6500\n",
      "the cat on the mat. and..........\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adamw.py:173\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    170\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    174\u001b[0m         group,\n\u001b[0;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m    176\u001b[0m         grads,\n\u001b[0;32m    177\u001b[0m         amsgrad,\n\u001b[0;32m    178\u001b[0m         exp_avgs,\n\u001b[0;32m    179\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    180\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    181\u001b[0m         state_steps,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    184\u001b[0m     adamw(\n\u001b[0;32m    185\u001b[0m         params_with_grad,\n\u001b[0;32m    186\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adamw.py:121\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    115\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    116\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    119\u001b[0m )\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    122\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m    125\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[0;32m    126\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[0;32m    127\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input sentence with [MASK]\n",
    "# input_sentence = \"The cat [MASK] on the mat.\"\n",
    "input_sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize input and get labels\n",
    "tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "labels = tokenized_input['input_ids'].clone()\n",
    "\n",
    "# Replace a token with [MASK] (e.g., the word \"sat\")\n",
    "masked_position = 4\n",
    "tokenized_input['input_ids'][0, masked_position] = tokenizer.mask_token_id\n",
    "\n",
    "for i in range(100000):\n",
    "    if i%100 == 0:\n",
    "        print(\"Epoch:\", i)\n",
    "        model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs)\n",
    "        print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n",
    "    # Forward pass\n",
    "    outputs = model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits[:, masked_position, :], labels[:, masked_position])\n",
    "\n",
    "    # Backward pass and parameter update\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "s4 = \"The cat [MASK] on the mat.\"\n",
    "model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff23730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
