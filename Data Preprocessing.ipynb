{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6df954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc527e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data info\n",
    "data_file = \"\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\"\n",
    "data_path = print(os.path.join('C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics', data_file))\n",
    "print(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28029b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d48dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text: brown.a.01 dog.n.01 Colour -1 entity.n.01 Sub -1 Sub +2 grey.a.01 dog.n.01 Colour -1 time.n.08 EQU now fight.v.01 Agent -4 Time -1 Location +1 snow.n.02 \n",
      "The roles masked are: ['Colour', 'Sub', 'Sub', 'Colour', 'Agent', 'Time', 'Location']\n",
      "\n",
      "Fully masked text: brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "\n",
      "Text with [MASK] for roles:\n",
      "brown.a.01 dog.n.01 [MASK] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [MASK] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [MASK] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [MASK] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [MASK] -4 [X] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [MASK] -1 [X] +1 snow.n.02 \n",
      "brown.a.01 dog.n.01 [X] -1 entity.n.01 [X] -1 [X] +2 grey.a.01 dog.n.01 [X] -1 time.n.08 EQU now fight.v.01 [X] -4 [X] -1 [MASK] +1 snow.n.02 \n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(file_path, save_path):\n",
    "    file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "    save_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\sentences.csv'\n",
    "    processed_text = \"\"\n",
    "    with open(file_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] != \"%\":\n",
    "                text = line\n",
    "                # remove extra whitespace\n",
    "                text = \" \".join(text.split()).strip()\n",
    "                # remove comments\n",
    "                comment_start = text.find(\"%\")\n",
    "                if comment_start > -1:\n",
    "                    text = text[:comment_start]\n",
    "                processed_text += text\n",
    "    print(\"Processed text:\", processed_text)\n",
    "    \n",
    "    # method to replace roles with \"[X]\" or [MASK]\n",
    "    roles = [] # fill in list when only specific roles (so, not Sub etc.)\n",
    "    if not roles:\n",
    "        # find roles by using -1 and +2, etc.\n",
    "        roles = re.findall('\\d (\\w+) [-+]', processed_text)\n",
    "        print(\"The roles masked are:\", roles)\n",
    "        \n",
    "    final_text = processed_text\n",
    "    for role in roles:\n",
    "        final_text = final_text.replace(role, \"[X]\")\n",
    "    print(\"\\nFully masked text:\", final_text)\n",
    "    \n",
    "    print(\"\\nText with [MASK] for roles:\")\n",
    "    instances = [m.start()-1 for m in re.finditer(\"[X]\", final_text)]\n",
    "    for instance in instances:\n",
    "        before = final_text[:instance]\n",
    "        after = final_text[instance:].replace(\"[X]\", \"[MASK]\", 1)\n",
    "        masked_text = before + after\n",
    "        print(masked_text)\n",
    "        \n",
    "preprocess_sentence(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9800c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Need to find: [\"Colour\", \"Sub\", \"Sub\", \"Colour\", \"Agent\", \"Time\", \"Location\"]\n",
    "s = 'brown.a.01 dog.n.01 Colour -1 entity.n.01 Sub -1 Sub +2 grey.a.01 dog.n.01 Colour -1 time.n.08 EQU now fight.v.01 Agent -4 Time -1 Location +1 snow.n.02 '\n",
    "\n",
    "# result = re.findall('\\d (.*?) [-+]\\d', s)\n",
    "# print(result)\n",
    "result = re.findall('\\d (\\w+) [-+]', s)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d08039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#letters = [\"Alef\", \"Ayin\", \"Bet\", \"Dalet\", \"Gimel\", \"He\", \"Het\", \"Kaf\", \"Kaf-final\", \"Lamed\", \"Mem\", \"Mem-medial\", \"Nun-final\", \"Nun-medial\", \"Pe\", \"Pe-final\", \"Qof\", \"Resh\", \"Samekh\", \"Shin\", \"Taw\", \"Tet\", \"Tsadi-final\", \"Tsadi-medial\", \"Waw\", \"Yod\", \"Zayin\"]\n",
    "# os.chdir(cwd)\n",
    "# r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "original_dataset_path = \"pmb-sample-4.0.0\\data\\en\\gold\"\n",
    "output_path = \"augment\"\n",
    "\n",
    "for classname in os.listdir(original_dataset_path):\n",
    "    class_input_dir = os.path.join(original_dataset_path, classname)\n",
    "    class_output_dir = os.path.join(output_path, classname)\n",
    "    # Create a new directory if it did not yet exist\n",
    "    if not os.path.exists(class_output_dir):\n",
    "        os.makedirs(class_output_dir)\n",
    "    nr = 0\n",
    "    size = len(os.listdir(class_input_dir))\n",
    "    for filename in os.listdir(class_input_dir):\n",
    "        if nr % int(size/10) == 0:\n",
    "            print(nr, \" out of \", size)\n",
    "        if True:#try:\n",
    "            f = os.path.join(class_input_dir,filename)\n",
    "            if os.path.isfile(f):\n",
    "                nr += 1\n",
    "                augment(f, classname, nr, class_output_dir)\n",
    "                cv2.destroyAllWindows()\n",
    "#                 break\n",
    "#         except:\n",
    "#             print(\"error\")\n",
    "#             break\n",
    "#             print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c5930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bbed55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m mask_indices \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked_indices\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    109\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 111\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1380\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()  \u001b[38;5;66;03m# -100 index = padding token\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     masked_lm_loss \u001b[38;5;241m=\u001b[39m loss_fct(prediction_scores\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1383\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, sentences, labels):\n",
    "#         self.sentences = sentences\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ids = self.sentences[idx]['input_ids']\n",
    "#         labels = self.labels[idx]\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         input_ids_tensor = torch.tensor(input_ids)\n",
    "#         labels_tensor = torch.tensor(labels)\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': input_ids_tensor,\n",
    "#             'labels': labels_tensor\n",
    "#         }\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define your dataset class\n",
    "class MaskedSentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, masked_indices, labels):\n",
    "        self.sentences = sentences\n",
    "        self.masked_indices = masked_indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.sentences_idx = tokenizer.encode(self.sentences[idx], add_special_tokens=True)\n",
    "        self.masked_indices_idx = self.masked_indices[idx]\n",
    "        self.labels_idx = tokenizer.encode(self.labels[idx], add_special_tokens=True)\n",
    "        \n",
    "#         self.sentences_tensor = torch.tensor(self.sentences_idx)\n",
    "#         self.masked_indices_tensor = torch.tensor(self.masked_indices_idx)\n",
    "#         self.labels_tensor = torch.tensor(self.labels_idx)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.sentences_idx),\n",
    "            'masked_indices': torch.tensor(self.masked_indices_idx),\n",
    "            'labels': torch.tensor(self.labels_idx)\n",
    "        }\n",
    "\n",
    "# Example data\n",
    "sentences = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
    "masked_indices = [[2], [1]]\n",
    "labels = [\"masked\", \"example\"]\n",
    "\n",
    "# Tokenize input sentences\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create dataset\n",
    "dataset = MaskedSentenceDataset(sentences, masked_indices, labels)\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define DataLoader and other training parameters\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    masked_indices = [item['masked_indices'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    # Adjust the padding_value according to your tokenizer\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'masked_indices': masked_indices,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you already have your MaskedSentenceDataset instance 'dataset'\n",
    "batch_size = 32  # Adjust the batch size according to your preferences\n",
    "\n",
    "# Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        mask_indices = batch['masked_indices']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation and optimization steps\n",
    "\n",
    "# Evaluation and inference steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb801c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the is blue. underside. underside. underside. underside. underside. underside. underside\n"
     ]
    }
   ],
   "source": [
    "# # Input sentence with a mask token [MASK]\n",
    "# input_sentence = \"The [MASK] is blue.\"\n",
    "\n",
    "# # Tokenize the input sentence\n",
    "# tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "\n",
    "\n",
    "# # Get the position of the masked token in the input\n",
    "# mask_token_index = (tokenized_input['input_ids'] == tokenizer.mask_token_id).nonzero().item()\n",
    "\n",
    "# # Forward pass to get predictions\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**tokenized_input)\n",
    "\n",
    "# # Get the predicted logits for the masked token\n",
    "# predictions = outputs.logits[:, mask_token_index, :]\n",
    "\n",
    "# # Get the predicted token ID (argmax or sampling)\n",
    "# predicted_token_id = torch.argmax(predictions).item()\n",
    "\n",
    "# # Convert the predicted token ID back to a word\n",
    "# predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "\n",
    "# print(f\"The predicted word for the mask is: {predicted_word}\")\n",
    "\n",
    "# model_inputs = tokenizer([\"ما لون السماء؟\"], return_tensors=\"pt\")\n",
    "\n",
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "model_inputs = tokenizer([s3], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a89df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7141d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f8c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "the cat on the mat. nothing. nothing.... nothing...\n",
      "Epoch: 100\n",
      "the cat on the mat............\n",
      "Epoch: 200\n",
      "the cat on the mat............\n",
      "Epoch: 300\n",
      "the cat on the mat............\n",
      "Epoch: 400\n",
      "the cat on the mat............\n",
      "Epoch: 500\n",
      "the cat on the mat. mom. mom. mom. and mom. mom.\n",
      "Epoch: 600\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 700\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 800\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 900\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 1000\n",
      "the cat on the mat. mom. mom. mom. my mom. mom.\n",
      "Epoch: 1100\n",
      "the cat on the mat. mom. mom. \" \". \".. \"\n",
      "Epoch: 1200\n",
      "the cat on the mat. mom. mom. \" \" \" \" \" \" \"\n",
      "Epoch: 1300\n",
      "the cat on the mat............\n",
      "Epoch: 1400\n",
      "the cat on the mat............\n",
      "Epoch: 1500\n",
      "the cat on the mat............\n",
      "Epoch: 1600\n",
      "the cat on the mat............\n",
      "Epoch: 1700\n",
      "the cat on the mat............\n",
      "Epoch: 1800\n",
      "the cat on the mat............\n",
      "Epoch: 1900\n",
      "the cat on the mat............\n",
      "Epoch: 2000\n",
      "the cat on the mat............\n",
      "Epoch: 2100\n",
      "the cat on the mat............\n",
      "Epoch: 2200\n",
      "the cat on the mat............\n",
      "Epoch: 2300\n",
      "the cat on the mat............\n",
      "Epoch: 2400\n",
      "the cat on the mat............\n",
      "Epoch: 2500\n",
      "the cat on the mat............\n",
      "Epoch: 2600\n",
      "the cat on the mat............\n",
      "Epoch: 2700\n",
      "the cat on the mat............\n",
      "Epoch: 2800\n",
      "the cat on the mat............\n",
      "Epoch: 2900\n",
      "the cat on the mat............\n",
      "Epoch: 3000\n",
      "the cat on the mat............\n",
      "Epoch: 3100\n",
      "the cat on the mat............\n",
      "Epoch: 3200\n",
      "the cat on the mat............\n",
      "Epoch: 3300\n",
      "the cat on the mat............\n",
      "Epoch: 3400\n",
      "the cat on the mat. and..........\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input sentence with [MASK]\n",
    "# input_sentence = \"The cat [MASK] on the mat.\"\n",
    "input_sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize input and get labels\n",
    "tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "labels = tokenized_input['input_ids'].clone()\n",
    "\n",
    "# Replace a token with [MASK] (e.g., the word \"sat\")\n",
    "masked_position = 4\n",
    "tokenized_input['input_ids'][0, masked_position] = tokenizer.mask_token_id\n",
    "\n",
    "for i in range(100000):\n",
    "    if i%100 == 0:\n",
    "        print(\"Epoch:\", i)\n",
    "        model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs)\n",
    "        print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n",
    "    # Forward pass\n",
    "    outputs = model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits[:, masked_position, :], labels[:, masked_position])\n",
    "\n",
    "    # Backward pass and parameter update\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd879e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "s4 = \"The cat [MASK] on the mat.\"\n",
    "model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e42347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
