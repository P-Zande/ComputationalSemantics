{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecbf2ee",
   "metadata": {},
   "source": [
    "# Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b43fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROLES/LABELS: Agent, Location, Topic, Patient, Theme, EMPTY\n",
    "# Tags: 0=EMPTY, 1=Agent, 2=Location, 3=Patient, 4=Theme, 5=Topic\n",
    "\n",
    "# Generate:\n",
    "# Whole sentence +\n",
    "# {'id': '0',\n",
    "#  'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#  'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n",
    "# }\n",
    "\n",
    "# THIS IS THE GOAL\n",
    "# Tags: 0=EMPTY, 1=Agent, 2=Location, 3=Patient, 4=Theme, 5=Topic\n",
    "# ner_tags = [1,1,1,1,1,1,1,0,0,2,2,2]\n",
    "# tokens = ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "# Example with one sentence:\n",
    "# Note: forward slashes for Linux and WSL, backward slashes for Windows\n",
    "# Windows example:\n",
    "# file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004'\n",
    "file_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-sample-4.0.0/data/en/gold/p00/d0004/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE GOAL\n",
    "# sentence = \"A brown dog and a grey dog are fighting in the snow\"\n",
    "# sentence_id = '0'\n",
    "# ner_tags = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2]\n",
    "# tokens = ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']\n",
    "mapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5}\n",
    "\n",
    "sentence = \"\"\n",
    "sentence_id = '0'\n",
    "ner_tags = [],\n",
    "tokens = []\n",
    "\n",
    "# Get the tokens from the tokenized sentence file\n",
    "with open(file_path+\"en.tok.off\") as file:\n",
    "    for line in file:\n",
    "        tokens.append(line.split()[-1])\n",
    "\n",
    "sentence = ' '.join(tokens)\n",
    "# Initially set all the tags as 0 (EMPTY)\n",
    "ner_tags = [0] * len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de12d2",
   "metadata": {},
   "source": [
    "## New class-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57dfbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCGNode:\n",
    "    def __init__(self, category = 'none', rule_type='none', parent=None, level = 0):\n",
    "        self.category = category # eg s\\np or np\n",
    "        self.rule_type = rule_type # fa or ba or conj\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.level = level\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([' ' * self.level, 'CCGNODE', ' ', self.category, ' ', self.rule_type, '\\n', '\\n'.join([repr(child) for child in self.children])])\n",
    "#         return 'CCGNODE: ' + str(id(self)) + '\\n ' + '\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\n",
    "\n",
    "class CCGToken:\n",
    "    def __init__(self, token, category, parent, assignedTag = '', verbnet = [], level = 0):\n",
    "        self.token = token\n",
    "        self.category = category\n",
    "        self.parent = parent\n",
    "        self.assignedTag = assignedTag\n",
    "        self.verbnet = verbnet\n",
    "        self.children = []\n",
    "        self.level = level\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([' ' * self.level, 'CCGTOKEN', ' ', self.token, ' ', self.category, ' ', self.assignedTag, ' ',' '.join(self.verbnet)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4552cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCGNODE none none\n",
      "\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A np/n  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown n/n  \n",
      "     CCGTOKEN dog n  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and conj  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a np/n  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey n/n  \n",
      "      CCGTOKEN dog n  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are (s:dcl\\np)/(s:ng\\np)  \n",
      "    CCGTOKEN fighting s:ng\\np  Agent\n",
      "   CCGNODE (s\\np)\\(s\\np) fa\n",
      "    CCGTOKEN in ((s\\np)\\(s\\np))/np  Location\n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN the np/n  \n",
      "     CCGTOKEN snow n  \n"
     ]
    }
   ],
   "source": [
    "token_idx = 0\n",
    "del(topNode)\n",
    "del(currentNode)\n",
    "topNode = None\n",
    "currentNode = None\n",
    "with open(file_path + \"en.parse.tags\") as file:\n",
    "    currentNode = None\n",
    "    skipping = True\n",
    "    previousLevel = 0\n",
    "    for line in file:\n",
    "        if skipping:\n",
    "            if line.startswith('ccg'):\n",
    "                skipping = False\n",
    "                topNode = CCGNode()\n",
    "                print(topNode)\n",
    "                currentNode = topNode\n",
    "            continue\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "#         print(line)\n",
    "        trimmedLine = line.lstrip()\n",
    "        nodeType, content = trimmedLine.split('(', 1)\n",
    "        category = content.split(',')[0]\n",
    "        if nodeType == 't':\n",
    "            verbnet = [r for r in mapping.keys() if r in line]\n",
    "            currentNode.children.append(CCGToken(tokens[token_idx], category = category, parent = currentNode, verbnet = verbnet, level = currentNode.level + 1))\n",
    "            token_idx += 1\n",
    "        else:\n",
    "            level = len(line) - len(trimmedLine)\n",
    "            if level > previousLevel: # This is a child of previous node\n",
    "                currentNode.children.append(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                currentNode = currentNode.children[-1]\n",
    "            elif level == previousLevel: # Sibling of the previous node; same parent\n",
    "                currentNode = currentNode.parent\n",
    "                currentNode.children.append(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                currentNode = currentNode.children[-1]\n",
    "            else: # Go back 1? level\n",
    "                currentNode = currentNode.parent.parent\n",
    "                currentNode.children.append(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                currentNode = currentNode.children[-1]\n",
    "                \n",
    "            previousLevel = level\n",
    "\n",
    "print(topNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed101314",
   "metadata": {},
   "source": [
    "## Old approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the roles for each token from the (parse/drs?)\n",
    "token_idx = 0\n",
    "with open(file_path + \"en.parse.tags\") as file:\n",
    "    # collect all the NP and corresponding words\n",
    "    NPs = []\n",
    "    for line in file:\n",
    "        if re.search('([fb]a\\(np),', line):\n",
    "            NPs.append([re.search('([fb]a\\(np),', line).group(1)])\n",
    "            print(\"Bla,\", NPs[-1])\n",
    "        \n",
    "        # Find a CCG for a token\n",
    "        if token_idx < len(tokens) and line.find(tokens[token_idx]) != -1: # TODO: fix to make sure single letter/small words found correctly\n",
    "            print(line)\n",
    "#             print(\"Contains token:\", tokens[token_idx], \"at position\", line.find(tokens[token_idx]))\n",
    "            \n",
    "            # Add the token to its closes NP\n",
    "            NPs[-1].append(tokens[token_idx])\n",
    "            \n",
    "            # Check whether any of the roles are in the sentence\n",
    "            roles = [r for r in mapping.keys() if r in line]\n",
    "            if roles:\n",
    "#                 print(line)\n",
    "                print(\"Has role:\", roles)\n",
    "                \n",
    "                # Find the CCG (using regex after \"t(\", finding last \\np or /np before ,)\n",
    "                ccg = re.search('([\\\\/\\\\\\][np]+),', line) # TODO: make sure I didn't think too simple for this\n",
    "                ccg = ccg.group(1)\n",
    "                print(\"CCG (last part):\", ccg)\n",
    "                \n",
    "                # Find whether role should be placed using forward/backward application\n",
    "                if ccg[0] == \"\\\\\": #backwards\n",
    "                    print(\"backwards\")\n",
    "                    # Look at all previous \"ba(np\" and find the one corresponding to this ccg\n",
    "                elif ccg[0] == \"\\/\": #forwards\n",
    "                    print(\"forwards\")\n",
    "                    # Look at all next \"fa(np\" and find the one corresponding to this ccg\n",
    "                    \n",
    "                # Find place of NP for role (\"fa(np\" or \"ba(np\")\n",
    "                # Find all word positions belonging to NP and assign their tokens the correct role?\n",
    "                # TODO: make sure it is fine to only look at NPs\n",
    "            token_idx += 1\n",
    "    print(\"NPs and their tokens:\", NPs)\n",
    "\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"ner_tags:\", ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = r\"t(s:ng\\np, 'fighting', [from:31, to:39, pos:'VBG', lemma:'fight', sem:'EXG', wordnet:'fight.v.01', verbnet:['Agent']])),\"\n",
    "print(re.search('([\\\\/\\\\\\][np]+),', txt))\n",
    "print(re.search('([\\\\/\\\\\\][np]+),', txt).group(1))\n",
    "# print(re.search('([\\\\/\\\\\\][np]+),', \"r\\'\"+txt+\"\\'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427c08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd4e4d5",
   "metadata": {},
   "source": [
    "# Graph/DRS data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33269896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(file_path, save_path):\n",
    "    file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "    save_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\sentences.csv'\n",
    "    processed_text = \"\"\n",
    "    with open(file_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] != \"%\":\n",
    "                text = line\n",
    "                # remove extra whitespace\n",
    "                text = \" \".join(text.split()).strip()\n",
    "                # remove comments\n",
    "                comment_start = text.find(\"%\")\n",
    "                if comment_start > -1:\n",
    "                    text = text[:comment_start]\n",
    "                processed_text += text\n",
    "    print(\"Processed text:\", processed_text)\n",
    "    \n",
    "    # method to replace roles with \"[X]\" or [MASK]\n",
    "    roles = [] # fill in list when only specific roles (so, not Sub etc.)\n",
    "    if not roles:\n",
    "        # find roles by using -1 and +2, etc.\n",
    "        roles = re.findall('\\d (\\w+) [-+]', processed_text)\n",
    "        print(\"The roles masked are:\", roles)\n",
    "        \n",
    "    final_text = processed_text\n",
    "    for role in roles:\n",
    "        final_text = final_text.replace(role, \"[X]\")\n",
    "    print(\"\\nFully masked text:\", final_text)\n",
    "    \n",
    "    print(\"\\nText with [MASK] for roles:\")\n",
    "    instances = [m.start()-1 for m in re.finditer(\"[X]\", final_text)]\n",
    "    for instance in instances:\n",
    "        before = final_text[:instance]\n",
    "        after = final_text[instance:].replace(\"[X]\", \"[MASK]\", 1)\n",
    "        masked_text = before + after\n",
    "        print(masked_text)\n",
    "        \n",
    "preprocess_sentence(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51894c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Need to find: [\"Colour\", \"Sub\", \"Sub\", \"Colour\", \"Agent\", \"Time\", \"Location\"]\n",
    "s = 'brown.a.01 dog.n.01 Colour -1 entity.n.01 Sub -1 Sub +2 grey.a.01 dog.n.01 Colour -1 time.n.08 EQU now fight.v.01 Agent -4 Time -1 Location +1 snow.n.02 '\n",
    "\n",
    "# result = re.findall('\\d (.*?) [-+]\\d', s)\n",
    "# print(result)\n",
    "result = re.findall('\\d (\\w+) [-+]', s)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#letters = [\"Alef\", \"Ayin\", \"Bet\", \"Dalet\", \"Gimel\", \"He\", \"Het\", \"Kaf\", \"Kaf-final\", \"Lamed\", \"Mem\", \"Mem-medial\", \"Nun-final\", \"Nun-medial\", \"Pe\", \"Pe-final\", \"Qof\", \"Resh\", \"Samekh\", \"Shin\", \"Taw\", \"Tet\", \"Tsadi-final\", \"Tsadi-medial\", \"Waw\", \"Yod\", \"Zayin\"]\n",
    "# os.chdir(cwd)\n",
    "# r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "original_dataset_path = \"pmb-sample-4.0.0\\data\\en\\gold\"\n",
    "output_path = \"augment\"\n",
    "\n",
    "for classname in os.listdir(original_dataset_path):\n",
    "    class_input_dir = os.path.join(original_dataset_path, classname)\n",
    "    class_output_dir = os.path.join(output_path, classname)\n",
    "    # Create a new directory if it did not yet exist\n",
    "    if not os.path.exists(class_output_dir):\n",
    "        os.makedirs(class_output_dir)\n",
    "    nr = 0\n",
    "    size = len(os.listdir(class_input_dir))\n",
    "    for filename in os.listdir(class_input_dir):\n",
    "        if nr % int(size/10) == 0:\n",
    "            print(nr, \" out of \", size)\n",
    "        if True:#try:\n",
    "            f = os.path.join(class_input_dir,filename)\n",
    "            if os.path.isfile(f):\n",
    "                nr += 1\n",
    "                augment(f, classname, nr, class_output_dir)\n",
    "                cv2.destroyAllWindows()\n",
    "#                 break\n",
    "#         except:\n",
    "#             print(\"error\")\n",
    "#             break\n",
    "#             print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66630635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28e0cf94",
   "metadata": {},
   "source": [
    "# Fill-mask with specified masks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce344ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, sentences, labels):\n",
    "#         self.sentences = sentences\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ids = self.sentences[idx]['input_ids']\n",
    "#         labels = self.labels[idx]\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         input_ids_tensor = torch.tensor(input_ids)\n",
    "#         labels_tensor = torch.tensor(labels)\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': input_ids_tensor,\n",
    "#             'labels': labels_tensor\n",
    "#         }\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define your dataset class\n",
    "class MaskedSentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, masked_indices, labels):\n",
    "        self.sentences = sentences\n",
    "        self.masked_indices = masked_indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.sentences_idx = tokenizer.encode(self.sentences[idx], add_special_tokens=True)\n",
    "        self.masked_indices_idx = self.masked_indices[idx]\n",
    "        self.labels_idx = tokenizer.encode(self.labels[idx], add_special_tokens=True)\n",
    "        \n",
    "#         self.sentences_tensor = torch.tensor(self.sentences_idx)\n",
    "#         self.masked_indices_tensor = torch.tensor(self.masked_indices_idx)\n",
    "#         self.labels_tensor = torch.tensor(self.labels_idx)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.sentences_idx),\n",
    "            'masked_indices': torch.tensor(self.masked_indices_idx),\n",
    "            'labels': torch.tensor(self.labels_idx)\n",
    "        }\n",
    "\n",
    "# Example data\n",
    "sentences = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
    "masked_indices = [[2], [1]]\n",
    "labels = [\"masked\", \"example\"]\n",
    "\n",
    "# Tokenize input sentences\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create dataset\n",
    "dataset = MaskedSentenceDataset(sentences, masked_indices, labels)\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define DataLoader and other training parameters\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    masked_indices = [item['masked_indices'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    # Adjust the padding_value according to your tokenizer\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'masked_indices': masked_indices,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you already have your MaskedSentenceDataset instance 'dataset'\n",
    "batch_size = 32  # Adjust the batch size according to your preferences\n",
    "\n",
    "# Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        mask_indices = batch['masked_indices']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation and optimization steps\n",
    "\n",
    "# Evaluation and inference steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1545381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input sentence with a mask token [MASK]\n",
    "# input_sentence = \"The [MASK] is blue.\"\n",
    "\n",
    "# # Tokenize the input sentence\n",
    "# tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "\n",
    "\n",
    "# # Get the position of the masked token in the input\n",
    "# mask_token_index = (tokenized_input['input_ids'] == tokenizer.mask_token_id).nonzero().item()\n",
    "\n",
    "# # Forward pass to get predictions\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**tokenized_input)\n",
    "\n",
    "# # Get the predicted logits for the masked token\n",
    "# predictions = outputs.logits[:, mask_token_index, :]\n",
    "\n",
    "# # Get the predicted token ID (argmax or sampling)\n",
    "# predicted_token_id = torch.argmax(predictions).item()\n",
    "\n",
    "# # Convert the predicted token ID back to a word\n",
    "# predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "\n",
    "# print(f\"The predicted word for the mask is: {predicted_word}\")\n",
    "\n",
    "# model_inputs = tokenizer([\"ما لون السماء؟\"], return_tensors=\"pt\")\n",
    "\n",
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "model_inputs = tokenizer([s3], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ca7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b480360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input sentence with [MASK]\n",
    "# input_sentence = \"The cat [MASK] on the mat.\"\n",
    "input_sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize input and get labels\n",
    "tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "labels = tokenized_input['input_ids'].clone()\n",
    "\n",
    "# Replace a token with [MASK] (e.g., the word \"sat\")\n",
    "masked_position = 4\n",
    "tokenized_input['input_ids'][0, masked_position] = tokenizer.mask_token_id\n",
    "\n",
    "for i in range(100000):\n",
    "    if i%100 == 0:\n",
    "        print(\"Epoch:\", i)\n",
    "        model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs)\n",
    "        print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n",
    "    # Forward pass\n",
    "    outputs = model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits[:, masked_position, :], labels[:, masked_position])\n",
    "\n",
    "    # Backward pass and parameter update\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "s4 = \"The cat [MASK] on the mat.\"\n",
    "model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff23730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
