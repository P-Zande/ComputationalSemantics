{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecbf2ee",
   "metadata": {},
   "source": [
    "# Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b43fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROLES/LABELS: Agent, Location, Topic, Patient, Theme, EMPTY\n",
    "# Tags: 0=EMPTY, 1=Agent, 2=Location, 3=Patient, 4=Theme, 5=Topic\n",
    "\n",
    "# Generate:\n",
    "# Whole sentence +\n",
    "# {'id': '0',\n",
    "#  'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#  'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n",
    "# }\n",
    "\n",
    "# THIS IS THE GOAL\n",
    "# Tags: 0=EMPTY, 1=Agent, 2=Location, 3=Patient, 4=Theme, 5=Topic\n",
    "# ner_tags = [1,1,1,1,1,1,1,0,0,2,2,2]\n",
    "# tokens = ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0aa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "# Example with one sentence:\n",
    "# Note: forward slashes for Linux and WSL, backward slashes for Windows\n",
    "# Windows example:\n",
    "# file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004'\n",
    "file_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-sample-4.0.0/data/en/gold/p00/d0004/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1abf505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE GOAL\n",
    "# sentence = \"A brown dog and a grey dog are fighting in the snow\"\n",
    "# sentence_id = '0'\n",
    "# ner_tags = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2]\n",
    "# tokens = ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow']\n",
    "mapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5}\n",
    "\n",
    "sentence = \"\"\n",
    "sentence_id = '0'\n",
    "ner_tags = [],\n",
    "tokens = []\n",
    "\n",
    "# Get the tokens from the tokenized sentence file\n",
    "with open(file_path+\"en.tok.off\") as file:\n",
    "    for line in file:\n",
    "        tokens.append(line.split()[-1])\n",
    "\n",
    "sentence = ' '.join(tokens)\n",
    "# Initially set all the tags as 0 (EMPTY)\n",
    "ner_tags = [0] * len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed17ca3",
   "metadata": {},
   "source": [
    "## New class-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3fbf67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCGNode:\n",
    "    def __init__(self, category = 'none', rule_type='none', parent=None, level = 0):\n",
    "        self.category = category # eg s\\np or np\n",
    "        self.rule_type = rule_type # fa or ba or conj\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.level = level\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([' ' * self.level, 'CCGNODE', ' ', self.category, ' ', self.rule_type, '\\n', '\\n'.join([repr(child) for child in self.children])])\n",
    "#         return 'CCGNODE: ' + str(id(self)) + '\\n ' + '\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\n",
    "\n",
    "class CCGToken:\n",
    "    def __init__(self, token: str, parent: CCGNode, assignedTag: str = '', verbnet = [], level = 0):\n",
    "        self.token = token\n",
    "        self.parent = parent\n",
    "        self.assignedTag = assignedTag\n",
    "        self.verbnet = verbnet\n",
    "        self.children = []\n",
    "        self.level = level\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ''.join([' ' * self.level, 'CCGTOKEN', ' ', self.token, ' ', self.assignedTag, ' ',' '.join(self.verbnet)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4aad1229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCGNODE none none\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "    CCGTOKEN fighting  Agent\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "    CCGTOKEN fighting  Agent\n",
      "   CCGNODE (s\\np)\\(s\\np) fa\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "    CCGTOKEN fighting  Agent\n",
      "   CCGNODE (s\\np)\\(s\\np) fa\n",
      "    CCGTOKEN in  Location\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "    CCGTOKEN fighting  Agent\n",
      "   CCGNODE (s\\np)\\(s\\np) fa\n",
      "    CCGTOKEN in  Location\n",
      "    CCGNODE np fa\n",
      "\n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "    CCGTOKEN fighting  Agent\n",
      "   CCGNODE (s\\np)\\(s\\np) fa\n",
      "    CCGTOKEN in  Location\n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN the  \n",
      "Current state:\n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "    CCGTOKEN fighting  Agent\n",
      "   CCGNODE (s\\np)\\(s\\np) fa\n",
      "    CCGTOKEN in  Location\n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN the  \n",
      "     CCGTOKEN snow  \n",
      "CCGNODE none none\n",
      " CCGNODE s:dcl ba\n",
      "  CCGNODE np ba\n",
      "   CCGNODE np fa\n",
      "    CCGTOKEN A  \n",
      "    CCGNODE n fa\n",
      "     CCGTOKEN brown  \n",
      "     CCGTOKEN dog  \n",
      "   CCGNODE np\\np conj\n",
      "    CCGTOKEN and  \n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN a  \n",
      "     CCGNODE n fa\n",
      "      CCGTOKEN grey  \n",
      "      CCGTOKEN dog  \n",
      "  CCGNODE s:dcl\\np ba\n",
      "   CCGNODE s:dcl\\np fa\n",
      "    CCGTOKEN are  \n",
      "    CCGTOKEN fighting  Agent\n",
      "   CCGNODE (s\\np)\\(s\\np) fa\n",
      "    CCGTOKEN in  Location\n",
      "    CCGNODE np fa\n",
      "     CCGTOKEN the  \n",
      "     CCGTOKEN snow  \n"
     ]
    }
   ],
   "source": [
    "token_idx = 0\n",
    "del(topNode)\n",
    "del(currentNode)\n",
    "topNode = None\n",
    "currentNode = None\n",
    "with open(file_path + \"en.parse.tags\") as file:\n",
    "    currentNode = None\n",
    "    skipping = True\n",
    "    previousLevel = 0\n",
    "    for line in file:\n",
    "        if skipping:\n",
    "            if line.startswith('ccg'):\n",
    "                skipping = False\n",
    "                topNode = CCGNode()\n",
    "                print(topNode)\n",
    "                currentNode = topNode\n",
    "            continue\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "#         print(line)\n",
    "        trimmedLine = line.lstrip()\n",
    "        nodeType, content = trimmedLine.split('(', 1)\n",
    "        if nodeType == 't':\n",
    "            verbnet = [r for r in mapping.keys() if r in line]\n",
    "            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet, level = currentNode.level + 1))\n",
    "            token_idx += 1\n",
    "        else:\n",
    "            level = len(line) - len(trimmedLine)\n",
    "            category = content.split(',')[0]\n",
    "            if level > previousLevel: # This is a child of previous node\n",
    "                currentNode.children.append(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                currentNode = currentNode.children[-1]\n",
    "            elif level == previousLevel: # Sibling of the previous node; same parent\n",
    "                currentNode = currentNode.parent\n",
    "                currentNode.children.append(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                currentNode = currentNode.children[-1]\n",
    "            else: # Go back 1? level\n",
    "                currentNode = currentNode.parent.parent\n",
    "                currentNode.children.append(CCGNode(category, nodeType, parent=currentNode, level = level))\n",
    "                currentNode = currentNode.children[-1]\n",
    "                \n",
    "            previousLevel = level\n",
    "        print(\"Current state:\")\n",
    "        print(topNode)\n",
    "\n",
    "print(topNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5c4f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "topNode = None\n",
    "currentNode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9602d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCGNODE: 140033530569680\n",
      " 140033539640768\n",
      " 96\n",
      " 140033539483392\n",
      " 96\n",
      " 140033539484016\n",
      " 96\n",
      " 140033539640960\n",
      " 0\n",
      " 140033539482288\n",
      " 0\n",
      " 140033539483440\n",
      " 0\n",
      " 140033530558256\n",
      " 0\n",
      " 140033530560128\n",
      " 0\n",
      " 140033530559984\n",
      " 0\n",
      " 140033530560368\n",
      " 0\n",
      " 140033530558400\n",
      " 96\n",
      " 140033530557248\n",
      " 0\n",
      " 140033539485552\n",
      " 0\n",
      " 140033530556864\n",
      " 0\n",
      " 140033530559024\n",
      " 0\n",
      " 140033530558688\n",
      " 0\n",
      " 140033539483632\n",
      " 96\n",
      " 140033539620576\n",
      " 96\n",
      " 140033539483104\n",
      " 96\n",
      " 140033539482336\n",
      " 0\n",
      " 140033539485120\n",
      " 0\n",
      " 140033539482096\n",
      " 0\n",
      " 140033600673056\n",
      " 0\n",
      " 140033530524576\n",
      " 0\n",
      " 140033530526400\n",
      " 0\n",
      " 140033530527456\n",
      " 0\n",
      " 140033530570112\n",
      " 96\n",
      " 140033530571168\n",
      " 0\n",
      " 140033530572224\n",
      " 0\n",
      " 140033530058016\n",
      " 0\n",
      " 140033530059840\n",
      " 0\n",
      " 140033530069152\n",
      " 0\n",
      " 140033601053984\n",
      " 96\n",
      " 140033530571984\n",
      " 96\n",
      " 140033530571600\n",
      " 96\n",
      " 140033530571360\n",
      " 0\n",
      " 140033600674112\n",
      " 0\n",
      " 140033530526688\n",
      " 0\n",
      " 140033530524768\n",
      " 0\n",
      " 140033539738016\n",
      " 0\n",
      " 140033539482576\n",
      " 0\n",
      " 140033539620240\n",
      " 0\n",
      " 140033539738544\n",
      " 96\n",
      " 140033539738160\n",
      " 0\n",
      " 140033539620432\n",
      " 0\n",
      " 140033530525440\n",
      " 0\n",
      " 140033530527600\n",
      " 0\n",
      " 140033530568768\n",
      " 0\n",
      " 140033530526352\n",
      " 96\n",
      " 140033530525488\n",
      " 96\n",
      " 140033530524192\n",
      " 96\n",
      " 140033539620720\n",
      " 0\n",
      " 140033530524288\n",
      " 0\n",
      " 140033530527168\n",
      " 0\n",
      " 140033530524672\n",
      " 0\n",
      " 140033530524864\n",
      " 0\n",
      " 140033539620384\n",
      " 0\n",
      " 140033539619952\n",
      " 0\n",
      " 140033530526112\n",
      " 96\n",
      " 140033539738064\n",
      " 0\n",
      " 140033601056384\n",
      " 0\n",
      " 140033530570400\n",
      " 0\n",
      " 140033530571312\n",
      " 0\n",
      " 140033530571456\n",
      " 0\n",
      " 140033601055808\n",
      " 96\n",
      " 140033539738928\n",
      " 96\n",
      " 140033539467296\n",
      " 96\n",
      " 140033600672576\n",
      " 0\n",
      " 140033539684096\n",
      " 0\n",
      " 140033539738496\n",
      " 0\n",
      " 140033539684336\n",
      " 0\n",
      " 140033539658416\n",
      " 0\n",
      " 140033530524960\n",
      " 0\n",
      " 140033530524096\n",
      " 0\n",
      " 140033539483056\n",
      " 96\n",
      " 140033539485456\n",
      " 0\n",
      " 140033530557392\n",
      " 0\n",
      " 140033530560080\n",
      " 0\n",
      " 140033530556528\n",
      " 0\n",
      " 140033530557200\n",
      " 0\n",
      " 140033530558736\n",
      " 96\n",
      " 140033539617600\n",
      " 96\n",
      " 140033530059360\n",
      " 96\n",
      " 140033530558112\n",
      " 0\n",
      " 140033539638560\n",
      " 0\n",
      " 140033530556768\n",
      " 0\n",
      " 140033539482720\n",
      " 0\n",
      " 140033539485024\n",
      " 0\n",
      " 140033539485264\n",
      " 0\n",
      " 140033539484064\n",
      " 0\n",
      " 140033539484976\n",
      " 96\n",
      " 140033539639472\n",
      " 0\n",
      " 140033530525872\n",
      " 0\n",
      " 140033539482960\n",
      " 0\n",
      " 140033530527024\n",
      " 0\n",
      " 140033539484352\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "bla = CCGNode('')\n",
    "\n",
    "print(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ff0613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCGNode', 'CCGToken', 'In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', '_i17', '_i18', '_i19', '_i2', '_i20', '_i21', '_i22', '_i23', '_i24', '_i25', '_i26', '_i27', '_i3', '_i4', '_i5', '_i6', '_i7', '_i8', '_i9', '_ih', '_ii', '_iii', '_oh', 'bla', 'category', 'content', 'currentNode', 'exit', 'file', 'file_path', 'get_ipython', 'level', 'line', 'mapping', 'ner_tags', 'nodeType', 'os', 'previousLevel', 'quit', 're', 'sentence', 'sentence_id', 'skipping', 'token_idx', 'tokens', 'topNode', 'trimmedLine', 'verbnet']\n",
      "{'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', \"import re\\nimport os\\n# Example with one sentence:\\n# Note: forward slashes for Linux and WSL, backward slashes for Windows\\n# Windows example:\\n# file_path = r'C:\\\\Users\\\\bikow\\\\Documents\\\\AI\\\\MSc\\\\Computational Semantics\\\\pmb-sample-4.0.0\\\\data\\\\en\\\\gold\\\\p00\\\\d0004'\\nfile_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-sample-4.0.0/data/en/gold/p00/d0004/'\", '# THIS IS THE GOAL\\n# sentence = \"A brown dog and a grey dog are fighting in the snow\"\\n# sentence_id = \\'0\\'\\n# ner_tags = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2]\\n# tokens = [\\'A\\', \\'brown\\', \\'dog\\', \\'and\\', \\'a\\', \\'grey\\', \\'dog\\', \\'are\\', \\'fighting\\', \\'in\\', \\'the\\', \\'snow\\']\\nmapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5}\\n\\nsentence = \"\"\\nsentence_id = \\'0\\'\\nner_tags = [],\\ntokens = []\\n\\n# Get the tokens from the tokenized sentence file\\nwith open(file_path+\"en.tok.off\") as file:\\n    for line in file:\\n        tokens.append(line.split()[-1])\\n\\nsentence = \\' \\'.join(tokens)\\n# Initially set all the tags as 0 (EMPTY)\\nner_tags = [0] * len(tokens)', \"class CCGNode:\\n    def __init__(self, category = 'none', rule_type='none', children=[], parent=None):\\n        self.category = category # eg s\\\\np or np\\n        self.rule_type = rule_type # fa or ba or conj\\n        self.children = children\\n        self.parent = parent\\n    \\n    def __repr__(self):\\n#         return 'CCGNODE: ' + self.category + self.rule_type + '\\\\n '.join([repr(child) for child in self.children])\\n        return 'CCGNODE: ' + str(id(self)) + '\\\\n ' + '\\\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\\n\\nclass CCGToken:\\n    def __init__(self, token: str, parent: CCGNode, assignedTag: str = '', verbnet = []):\\n        self.token = token\\n        self.parent = parent\\n        self.assignedTag = assignedTag\\n        self.verbnet = verbnet\\n        self.children = []\\n    \\n    def __repr__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\\n    \\n    def __str__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\", 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'print(dir())', 'del(topNode)', 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'del(CCGNode)', 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', \"class CCGNode:\\n    def __init__(self, category = 'none', rule_type='none', children=[], parent=None):\\n        self.category = category # eg s\\\\np or np\\n        self.rule_type = rule_type # fa or ba or conj\\n        self.children = children\\n        self.parent = parent\\n    \\n    def __repr__(self):\\n#         return 'CCGNODE: ' + self.category + self.rule_type + '\\\\n '.join([repr(child) for child in self.children])\\n        return 'CCGNODE: ' + str(id(self)) + '\\\\n ' + '\\\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\\n\\nclass CCGToken:\\n    def __init__(self, token: str, parent: CCGNode, assignedTag: str = '', verbnet = []):\\n        self.token = token\\n        self.parent = parent\\n        self.assignedTag = assignedTag\\n        self.verbnet = verbnet\\n        self.children = []\\n    \\n    def __repr__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\\n    \\n    def __str__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\", 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'print(dir())\\nprint(type(x) for x in dir())', 'print(dir())\\nbla = dir()\\nprint(type(x) for x in bla)', 'print(dir())\\nbla = dir()\\nprint([type(x) for x in bla])', 'print(type(CCGNode))', 'print(type(level))', 'print(dir())\\nbla = dir()\\nprint([type(x) for x in bla])', 'print(type(currentNode))', 'print(tokens)', 'print(CCGNode)', 'print(dir())', 'print(CCGToken)', 'print(dir())\\nprint(globals())', 'print(dir())\\nprint(globals())', 'topNode = None\\ncurrentNode = None', 'print(dir())\\nprint(globals())'], '_oh': {}, '_dh': ['/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/ComputationalSemantics'], 'In': ['', \"import re\\nimport os\\n# Example with one sentence:\\n# Note: forward slashes for Linux and WSL, backward slashes for Windows\\n# Windows example:\\n# file_path = r'C:\\\\Users\\\\bikow\\\\Documents\\\\AI\\\\MSc\\\\Computational Semantics\\\\pmb-sample-4.0.0\\\\data\\\\en\\\\gold\\\\p00\\\\d0004'\\nfile_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-sample-4.0.0/data/en/gold/p00/d0004/'\", '# THIS IS THE GOAL\\n# sentence = \"A brown dog and a grey dog are fighting in the snow\"\\n# sentence_id = \\'0\\'\\n# ner_tags = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2]\\n# tokens = [\\'A\\', \\'brown\\', \\'dog\\', \\'and\\', \\'a\\', \\'grey\\', \\'dog\\', \\'are\\', \\'fighting\\', \\'in\\', \\'the\\', \\'snow\\']\\nmapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5}\\n\\nsentence = \"\"\\nsentence_id = \\'0\\'\\nner_tags = [],\\ntokens = []\\n\\n# Get the tokens from the tokenized sentence file\\nwith open(file_path+\"en.tok.off\") as file:\\n    for line in file:\\n        tokens.append(line.split()[-1])\\n\\nsentence = \\' \\'.join(tokens)\\n# Initially set all the tags as 0 (EMPTY)\\nner_tags = [0] * len(tokens)', \"class CCGNode:\\n    def __init__(self, category = 'none', rule_type='none', children=[], parent=None):\\n        self.category = category # eg s\\\\np or np\\n        self.rule_type = rule_type # fa or ba or conj\\n        self.children = children\\n        self.parent = parent\\n    \\n    def __repr__(self):\\n#         return 'CCGNODE: ' + self.category + self.rule_type + '\\\\n '.join([repr(child) for child in self.children])\\n        return 'CCGNODE: ' + str(id(self)) + '\\\\n ' + '\\\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\\n\\nclass CCGToken:\\n    def __init__(self, token: str, parent: CCGNode, assignedTag: str = '', verbnet = []):\\n        self.token = token\\n        self.parent = parent\\n        self.assignedTag = assignedTag\\n        self.verbnet = verbnet\\n        self.children = []\\n    \\n    def __repr__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\\n    \\n    def __str__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\", 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'print(dir())', 'del(topNode)', 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'del(CCGNode)', 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', \"class CCGNode:\\n    def __init__(self, category = 'none', rule_type='none', children=[], parent=None):\\n        self.category = category # eg s\\\\np or np\\n        self.rule_type = rule_type # fa or ba or conj\\n        self.children = children\\n        self.parent = parent\\n    \\n    def __repr__(self):\\n#         return 'CCGNODE: ' + self.category + self.rule_type + '\\\\n '.join([repr(child) for child in self.children])\\n        return 'CCGNODE: ' + str(id(self)) + '\\\\n ' + '\\\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\\n\\nclass CCGToken:\\n    def __init__(self, token: str, parent: CCGNode, assignedTag: str = '', verbnet = []):\\n        self.token = token\\n        self.parent = parent\\n        self.assignedTag = assignedTag\\n        self.verbnet = verbnet\\n        self.children = []\\n    \\n    def __repr__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\\n    \\n    def __str__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\", 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'print(dir())\\nprint(type(x) for x in dir())', 'print(dir())\\nbla = dir()\\nprint(type(x) for x in bla)', 'print(dir())\\nbla = dir()\\nprint([type(x) for x in bla])', 'print(type(CCGNode))', 'print(type(level))', 'print(dir())\\nbla = dir()\\nprint([type(x) for x in bla])', 'print(type(currentNode))', 'print(tokens)', 'print(CCGNode)', 'print(dir())', 'print(CCGToken)', 'print(dir())\\nprint(globals())', 'print(dir())\\nprint(globals())', 'topNode = None\\ncurrentNode = None', 'print(dir())\\nprint(globals())'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f5c1d0699d0>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f5c1d0867f0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f5c1d0867f0>, '_': '', '__': '', '___': '', '_i': 'topNode = None\\ncurrentNode = None', '_ii': 'print(dir())\\nprint(globals())', '_iii': 'print(dir())\\nprint(globals())', '_i1': \"import re\\nimport os\\n# Example with one sentence:\\n# Note: forward slashes for Linux and WSL, backward slashes for Windows\\n# Windows example:\\n# file_path = r'C:\\\\Users\\\\bikow\\\\Documents\\\\AI\\\\MSc\\\\Computational Semantics\\\\pmb-sample-4.0.0\\\\data\\\\en\\\\gold\\\\p00\\\\d0004'\\nfile_path = r'/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-sample-4.0.0/data/en/gold/p00/d0004/'\", 're': <module 're' from '/usr/lib/python3.8/re.py'>, 'os': <module 'os' from '/usr/lib/python3.8/os.py'>, 'file_path': '/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-sample-4.0.0/data/en/gold/p00/d0004/', '_i2': '# THIS IS THE GOAL\\n# sentence = \"A brown dog and a grey dog are fighting in the snow\"\\n# sentence_id = \\'0\\'\\n# ner_tags = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2]\\n# tokens = [\\'A\\', \\'brown\\', \\'dog\\', \\'and\\', \\'a\\', \\'grey\\', \\'dog\\', \\'are\\', \\'fighting\\', \\'in\\', \\'the\\', \\'snow\\']\\nmapping = {\"Agent\": 1, \"Location\": 2, \"Patient\": 3, \"Theme\": 4, \"Topic\":5}\\n\\nsentence = \"\"\\nsentence_id = \\'0\\'\\nner_tags = [],\\ntokens = []\\n\\n# Get the tokens from the tokenized sentence file\\nwith open(file_path+\"en.tok.off\") as file:\\n    for line in file:\\n        tokens.append(line.split()[-1])\\n\\nsentence = \\' \\'.join(tokens)\\n# Initially set all the tags as 0 (EMPTY)\\nner_tags = [0] * len(tokens)', 'mapping': {'Agent': 1, 'Location': 2, 'Patient': 3, 'Theme': 4, 'Topic': 5}, 'sentence': 'A brown dog and a grey dog are fighting in the snow', 'sentence_id': '0', 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['A', 'brown', 'dog', 'and', 'a', 'grey', 'dog', 'are', 'fighting', 'in', 'the', 'snow'], 'file': <_io.TextIOWrapper name='/mnt/c/Users/perry/Documents/uni/Master/CompSem/project/pmb-sample-4.0.0/data/en/gold/p00/d0004/en.parse.tags' mode='r' encoding='UTF-8'>, 'line': '\\n', '_i3': \"class CCGNode:\\n    def __init__(self, category = 'none', rule_type='none', children=[], parent=None):\\n        self.category = category # eg s\\\\np or np\\n        self.rule_type = rule_type # fa or ba or conj\\n        self.children = children\\n        self.parent = parent\\n    \\n    def __repr__(self):\\n#         return 'CCGNODE: ' + self.category + self.rule_type + '\\\\n '.join([repr(child) for child in self.children])\\n        return 'CCGNODE: ' + str(id(self)) + '\\\\n ' + '\\\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\\n\\nclass CCGToken:\\n    def __init__(self, token: str, parent: CCGNode, assignedTag: str = '', verbnet = []):\\n        self.token = token\\n        self.parent = parent\\n        self.assignedTag = assignedTag\\n        self.verbnet = verbnet\\n        self.children = []\\n    \\n    def __repr__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\\n    \\n    def __str__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\", 'CCGToken': <class '__main__.CCGToken'>, '_i4': 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'token_idx': 12, 'currentNode': None, 'skipping': False, 'previousLevel': 5, 'trimmedLine': \"t(n, 'snow', [from:47, to:51, pos:'NN', lemma:'snow', sem:'CON', wordnet:'snow.n.02'])))))).\\n\", 'level': 5, 'nodeType': 't', 'content': \"n, 'snow', [from:47, to:51, pos:'NN', lemma:'snow', sem:'CON', wordnet:'snow.n.02'])))))).\\n\", 'category': 's:dcl\\\\np', 'verbnet': [], '_i5': 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', '_i6': 'print(dir())', '_i7': 'del(topNode)', '_i8': 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', 'topNode': None, '_i9': 'del(CCGNode)', '_i10': 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', '_i11': \"class CCGNode:\\n    def __init__(self, category = 'none', rule_type='none', children=[], parent=None):\\n        self.category = category # eg s\\\\np or np\\n        self.rule_type = rule_type # fa or ba or conj\\n        self.children = children\\n        self.parent = parent\\n    \\n    def __repr__(self):\\n#         return 'CCGNODE: ' + self.category + self.rule_type + '\\\\n '.join([repr(child) for child in self.children])\\n        return 'CCGNODE: ' + str(id(self)) + '\\\\n ' + '\\\\n '.join([f(child) for child in self.children for f in (lambda child: str(id(child)), lambda child: str(len(child.children)))])\\n\\nclass CCGToken:\\n    def __init__(self, token: str, parent: CCGNode, assignedTag: str = '', verbnet = []):\\n        self.token = token\\n        self.parent = parent\\n        self.assignedTag = assignedTag\\n        self.verbnet = verbnet\\n        self.children = []\\n    \\n    def __repr__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\\n    \\n    def __str__(self):\\n        return ', '.join(['CCGTOKEN', self.token, self.assignedTag,' '.join(self.verbnet)])\", 'CCGNode': <class '__main__.CCGNode'>, '_i12': 'token_idx = 0\\ntopNode = None\\ncurrentNode = None\\nwith open(file_path + \"en.parse.tags\") as file:\\n    currentNode = None\\n    skipping = True\\n    previousLevel = 0\\n    for line in file:\\n        if skipping:\\n            if line.startswith(\\'ccg\\'):\\n                skipping = False\\n                topNode = CCGNode()\\n                print(topNode)\\n                currentNode = topNode\\n            continue\\n        if line == \\'\\\\n\\':\\n            continue\\n#         print(line)\\n        trimmedLine = line.lstrip()\\n        level = len(line) - len(trimmedLine)\\n        nodeType, content = trimmedLine.split(\\'(\\', 1)\\n        print(nodeType)\\n        print(content)\\n        if nodeType == \\'t\\':\\n            verbnet = [r for r in mapping.keys() if r in line]\\n            print(verbnet)\\n            currentNode.children.append(CCGToken(tokens[token_idx], parent = currentNode, verbnet = verbnet))\\n            token_idx += 1\\n        else:\\n            if level > previousLevel: # This is a child of previous node\\n                category = content.split(\\',\\')[0]\\n                currentNode.children.append(CCGNode(category, nodeType, parent=None))\\n#                 currentNode = currentNode.children[-1]\\n        \\n        previousLevel = level\\n        print(\"Current state:\")\\n        print(topNode)\\n\\nprint(topNode)', '_i13': 'print(dir())\\nprint(type(x) for x in dir())', '_i14': 'print(dir())\\nbla = dir()\\nprint(type(x) for x in bla)', 'bla': ['CCGNode', 'CCGToken', 'In', 'Out', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', '_i17', '_i18', '_i2', '_i3', '_i4', '_i5', '_i6', '_i7', '_i8', '_i9', '_ih', '_ii', '_iii', '_oh', 'bla', 'category', 'content', 'currentNode', 'exit', 'file', 'file_path', 'get_ipython', 'level', 'line', 'mapping', 'ner_tags', 'nodeType', 'os', 'previousLevel', 'quit', 're', 'sentence', 'sentence_id', 'skipping', 'token_idx', 'tokens', 'topNode', 'trimmedLine', 'verbnet'], '_i15': 'print(dir())\\nbla = dir()\\nprint([type(x) for x in bla])', '_i16': 'print(type(CCGNode))', '_i17': 'print(type(level))', '_i18': 'print(dir())\\nbla = dir()\\nprint([type(x) for x in bla])', '_i19': 'print(type(currentNode))', '_i20': 'print(tokens)', '_i21': 'print(CCGNode)', '_i22': 'print(dir())', '_i23': 'print(CCGToken)', '_i24': 'print(dir())\\nprint(globals())', '_i25': 'print(dir())\\nprint(globals())', '_i26': 'topNode = None\\ncurrentNode = None', '_i27': 'print(dir())\\nprint(globals())'}\n"
     ]
    }
   ],
   "source": [
    "print(dir())\n",
    "print(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcfc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CCGToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57a446",
   "metadata": {},
   "source": [
    "## Old approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the roles for each token from the (parse/drs?)\n",
    "token_idx = 0\n",
    "with open(file_path + \"en.parse.tags\") as file:\n",
    "    # collect all the NP and corresponding words\n",
    "    NPs = []\n",
    "    for line in file:\n",
    "        if re.search('([fb]a\\(np),', line):\n",
    "            NPs.append([re.search('([fb]a\\(np),', line).group(1)])\n",
    "            print(\"Bla,\", NPs[-1])\n",
    "        \n",
    "        # Find a CCG for a token\n",
    "        if token_idx < len(tokens) and line.find(tokens[token_idx]) != -1: # TODO: fix to make sure single letter/small words found correctly\n",
    "            print(line)\n",
    "#             print(\"Contains token:\", tokens[token_idx], \"at position\", line.find(tokens[token_idx]))\n",
    "            \n",
    "            # Add the token to its closes NP\n",
    "            NPs[-1].append(tokens[token_idx])\n",
    "            \n",
    "            # Check whether any of the roles are in the sentence\n",
    "            roles = [r for r in mapping.keys() if r in line]\n",
    "            if roles:\n",
    "#                 print(line)\n",
    "                print(\"Has role:\", roles)\n",
    "                \n",
    "                # Find the CCG (using regex after \"t(\", finding last \\np or /np before ,)\n",
    "                ccg = re.search('([\\\\/\\\\\\][np]+),', line) # TODO: make sure I didn't think too simple for this\n",
    "                ccg = ccg.group(1)\n",
    "                print(\"CCG (last part):\", ccg)\n",
    "                \n",
    "                # Find whether role should be placed using forward/backward application\n",
    "                if ccg[0] == \"\\\\\": #backwards\n",
    "                    print(\"backwards\")\n",
    "                    # Look at all previous \"ba(np\" and find the one corresponding to this ccg\n",
    "                elif ccg[0] == \"\\/\": #forwards\n",
    "                    print(\"forwards\")\n",
    "                    # Look at all next \"fa(np\" and find the one corresponding to this ccg\n",
    "                    \n",
    "                # Find place of NP for role (\"fa(np\" or \"ba(np\")\n",
    "                # Find all word positions belonging to NP and assign their tokens the correct role?\n",
    "                # TODO: make sure it is fine to only look at NPs\n",
    "            token_idx += 1\n",
    "    print(\"NPs and their tokens:\", NPs)\n",
    "\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"ner_tags:\", ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = r\"t(s:ng\\np, 'fighting', [from:31, to:39, pos:'VBG', lemma:'fight', sem:'EXG', wordnet:'fight.v.01', verbnet:['Agent']])),\"\n",
    "print(re.search('([\\\\/\\\\\\][np]+),', txt))\n",
    "print(re.search('([\\\\/\\\\\\][np]+),', txt).group(1))\n",
    "# print(re.search('([\\\\/\\\\\\][np]+),', \"r\\'\"+txt+\"\\'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427c08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd4e4d5",
   "metadata": {},
   "source": [
    "# Graph/DRS data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33269896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(file_path, save_path):\n",
    "    file_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "    save_path = r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\sentences.csv'\n",
    "    processed_text = \"\"\n",
    "    with open(file_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] != \"%\":\n",
    "                text = line\n",
    "                # remove extra whitespace\n",
    "                text = \" \".join(text.split()).strip()\n",
    "                # remove comments\n",
    "                comment_start = text.find(\"%\")\n",
    "                if comment_start > -1:\n",
    "                    text = text[:comment_start]\n",
    "                processed_text += text\n",
    "    print(\"Processed text:\", processed_text)\n",
    "    \n",
    "    # method to replace roles with \"[X]\" or [MASK]\n",
    "    roles = [] # fill in list when only specific roles (so, not Sub etc.)\n",
    "    if not roles:\n",
    "        # find roles by using -1 and +2, etc.\n",
    "        roles = re.findall('\\d (\\w+) [-+]', processed_text)\n",
    "        print(\"The roles masked are:\", roles)\n",
    "        \n",
    "    final_text = processed_text\n",
    "    for role in roles:\n",
    "        final_text = final_text.replace(role, \"[X]\")\n",
    "    print(\"\\nFully masked text:\", final_text)\n",
    "    \n",
    "    print(\"\\nText with [MASK] for roles:\")\n",
    "    instances = [m.start()-1 for m in re.finditer(\"[X]\", final_text)]\n",
    "    for instance in instances:\n",
    "        before = final_text[:instance]\n",
    "        after = final_text[instance:].replace(\"[X]\", \"[MASK]\", 1)\n",
    "        masked_text = before + after\n",
    "        print(masked_text)\n",
    "        \n",
    "preprocess_sentence(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51894c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Need to find: [\"Colour\", \"Sub\", \"Sub\", \"Colour\", \"Agent\", \"Time\", \"Location\"]\n",
    "s = 'brown.a.01 dog.n.01 Colour -1 entity.n.01 Sub -1 Sub +2 grey.a.01 dog.n.01 Colour -1 time.n.08 EQU now fight.v.01 Agent -4 Time -1 Location +1 snow.n.02 '\n",
    "\n",
    "# result = re.findall('\\d (.*?) [-+]\\d', s)\n",
    "# print(result)\n",
    "result = re.findall('\\d (\\w+) [-+]', s)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#letters = [\"Alef\", \"Ayin\", \"Bet\", \"Dalet\", \"Gimel\", \"He\", \"Het\", \"Kaf\", \"Kaf-final\", \"Lamed\", \"Mem\", \"Mem-medial\", \"Nun-final\", \"Nun-medial\", \"Pe\", \"Pe-final\", \"Qof\", \"Resh\", \"Samekh\", \"Shin\", \"Taw\", \"Tet\", \"Tsadi-final\", \"Tsadi-medial\", \"Waw\", \"Yod\", \"Zayin\"]\n",
    "# os.chdir(cwd)\n",
    "# r'C:\\Users\\bikow\\Documents\\AI\\MSc\\Computational Semantics\\pmb-sample-4.0.0\\data\\en\\gold\\p00\\d0004\\en.drs.sbn'\n",
    "original_dataset_path = \"pmb-sample-4.0.0\\data\\en\\gold\"\n",
    "output_path = \"augment\"\n",
    "\n",
    "for classname in os.listdir(original_dataset_path):\n",
    "    class_input_dir = os.path.join(original_dataset_path, classname)\n",
    "    class_output_dir = os.path.join(output_path, classname)\n",
    "    # Create a new directory if it did not yet exist\n",
    "    if not os.path.exists(class_output_dir):\n",
    "        os.makedirs(class_output_dir)\n",
    "    nr = 0\n",
    "    size = len(os.listdir(class_input_dir))\n",
    "    for filename in os.listdir(class_input_dir):\n",
    "        if nr % int(size/10) == 0:\n",
    "            print(nr, \" out of \", size)\n",
    "        if True:#try:\n",
    "            f = os.path.join(class_input_dir,filename)\n",
    "            if os.path.isfile(f):\n",
    "                nr += 1\n",
    "                augment(f, classname, nr, class_output_dir)\n",
    "                cv2.destroyAllWindows()\n",
    "#                 break\n",
    "#         except:\n",
    "#             print(\"error\")\n",
    "#             break\n",
    "#             print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66630635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28e0cf94",
   "metadata": {},
   "source": [
    "# Fill-mask with specified masks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce344ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, sentences, labels):\n",
    "#         self.sentences = sentences\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ids = self.sentences[idx]['input_ids']\n",
    "#         labels = self.labels[idx]\n",
    "\n",
    "#         # Convert to tensors\n",
    "#         input_ids_tensor = torch.tensor(input_ids)\n",
    "#         labels_tensor = torch.tensor(labels)\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': input_ids_tensor,\n",
    "#             'labels': labels_tensor\n",
    "#         }\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define your dataset class\n",
    "class MaskedSentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, masked_indices, labels):\n",
    "        self.sentences = sentences\n",
    "        self.masked_indices = masked_indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.sentences_idx = tokenizer.encode(self.sentences[idx], add_special_tokens=True)\n",
    "        self.masked_indices_idx = self.masked_indices[idx]\n",
    "        self.labels_idx = tokenizer.encode(self.labels[idx], add_special_tokens=True)\n",
    "        \n",
    "#         self.sentences_tensor = torch.tensor(self.sentences_idx)\n",
    "#         self.masked_indices_tensor = torch.tensor(self.masked_indices_idx)\n",
    "#         self.labels_tensor = torch.tensor(self.labels_idx)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.sentences_idx),\n",
    "            'masked_indices': torch.tensor(self.masked_indices_idx),\n",
    "            'labels': torch.tensor(self.labels_idx)\n",
    "        }\n",
    "\n",
    "# Example data\n",
    "sentences = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
    "masked_indices = [[2], [1]]\n",
    "labels = [\"masked\", \"example\"]\n",
    "\n",
    "# Tokenize input sentences\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create dataset\n",
    "dataset = MaskedSentenceDataset(sentences, masked_indices, labels)\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define DataLoader and other training parameters\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    masked_indices = [item['masked_indices'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    # Adjust the padding_value according to your tokenizer\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'masked_indices': masked_indices,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you already have your MaskedSentenceDataset instance 'dataset'\n",
    "batch_size = 32  # Adjust the batch size according to your preferences\n",
    "\n",
    "# Create DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        mask_indices = batch['masked_indices']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation and optimization steps\n",
    "\n",
    "# Evaluation and inference steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1545381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input sentence with a mask token [MASK]\n",
    "# input_sentence = \"The [MASK] is blue.\"\n",
    "\n",
    "# # Tokenize the input sentence\n",
    "# tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "\n",
    "\n",
    "# # Get the position of the masked token in the input\n",
    "# mask_token_index = (tokenized_input['input_ids'] == tokenizer.mask_token_id).nonzero().item()\n",
    "\n",
    "# # Forward pass to get predictions\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**tokenized_input)\n",
    "\n",
    "# # Get the predicted logits for the masked token\n",
    "# predictions = outputs.logits[:, mask_token_index, :]\n",
    "\n",
    "# # Get the predicted token ID (argmax or sampling)\n",
    "# predicted_token_id = torch.argmax(predictions).item()\n",
    "\n",
    "# # Convert the predicted token ID back to a word\n",
    "# predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "\n",
    "# print(f\"The predicted word for the mask is: {predicted_word}\")\n",
    "\n",
    "# model_inputs = tokenizer([\"ما لون السماء؟\"], return_tensors=\"pt\")\n",
    "\n",
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "model_inputs = tokenizer([s3], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ca7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b480360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input sentence with [MASK]\n",
    "# input_sentence = \"The cat [MASK] on the mat.\"\n",
    "input_sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize input and get labels\n",
    "tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
    "labels = tokenized_input['input_ids'].clone()\n",
    "\n",
    "# Replace a token with [MASK] (e.g., the word \"sat\")\n",
    "masked_position = 4\n",
    "tokenized_input['input_ids'][0, masked_position] = tokenizer.mask_token_id\n",
    "\n",
    "for i in range(100000):\n",
    "    if i%100 == 0:\n",
    "        print(\"Epoch:\", i)\n",
    "        model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs)\n",
    "        print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n",
    "    # Forward pass\n",
    "    outputs = model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits[:, masked_position, :], labels[:, masked_position])\n",
    "\n",
    "    # Backward pass and parameter update\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"This is [MASK] sample sentence.\"\n",
    "s2 = \"Another [MASK] sentence.\"\n",
    "s3 = \"The [MASK] is blue.\"\n",
    "s4 = \"The cat [MASK] on the mat.\"\n",
    "model_inputs = tokenizer([s4], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff23730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
